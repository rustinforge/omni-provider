{"version":3,"sources":["../src/models.ts","../src/provider.ts","../src/version.ts","../src/config/index.ts","../src/stats.ts","../src/proxy.ts","../src/index.ts"],"sourcesContent":["/**\n * Models Configuration\n * \n * Supported models and aliases for OmniLLM in OpenClaw format.\n */\n\nimport type { ModelDefinitionConfig, ModelProviderConfig } from \"./types/openclaw.js\";\nimport type { LLMProvider, ModelInfo, ModelAlias } from \"./types.js\";\n\n// Internal model definitions (legacy format for compatibility)\nexport const MODELS: Record<string, ModelInfo> = {\n  // OpenAI\n  \"gpt-4o\": {\n    id: \"gpt-4o\",\n    name: \"GPT-4o\",\n    provider: \"openai\",\n    contextWindow: 128000,\n    maxOutputTokens: 16384,\n    capabilities: { streaming: true, functionCalling: true, vision: true },\n    pricing: { input: 5.0, output: 15.0 },\n  },\n  \"gpt-4o-mini\": {\n    id: \"gpt-4o-mini\",\n    name: \"GPT-4o Mini\",\n    provider: \"openai\",\n    contextWindow: 128000,\n    maxOutputTokens: 16384,\n    capabilities: { streaming: true, functionCalling: true, vision: true },\n    pricing: { input: 0.15, output: 0.6 },\n  },\n  \"gpt-5\": {\n    id: \"gpt-5\",\n    name: \"GPT-5\",\n    provider: \"openai\",\n    contextWindow: 200000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true, vision: true, reasoning: true },\n    pricing: { input: 10.0, output: 30.0 },\n  },\n  \"o3\": {\n    id: \"o3\",\n    name: \"OpenAI o3\",\n    provider: \"openai\",\n    contextWindow: 200000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: false, functionCalling: true, reasoning: true },\n    pricing: { input: 10.0, output: 40.0 },\n  },\n  \"o3-mini\": {\n    id: \"o3-mini\",\n    name: \"OpenAI o3-mini\",\n    provider: \"openai\",\n    contextWindow: 128000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: false, functionCalling: true, reasoning: true },\n    pricing: { input: 1.1, output: 4.4 },\n  },\n\n  // Anthropic\n  \"claude-sonnet-4\": {\n    id: \"claude-sonnet-4\",\n    name: \"Claude Sonnet 4\",\n    provider: \"anthropic\",\n    contextWindow: 200000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true, vision: true },\n    pricing: { input: 3.0, output: 15.0 },\n  },\n  \"claude-opus-4\": {\n    id: \"claude-opus-4\",\n    name: \"Claude Opus 4\",\n    provider: \"anthropic\",\n    contextWindow: 200000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true, vision: true },\n    pricing: { input: 15.0, output: 75.0 },\n  },\n  \"claude-haiku-4\": {\n    id: \"claude-haiku-4\",\n    name: \"Claude Haiku 4\",\n    provider: \"anthropic\",\n    contextWindow: 200000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true, vision: true },\n    pricing: { input: 0.8, output: 4.0 },\n  },\n\n  // Google\n  \"gemini-2.5-pro\": {\n    id: \"gemini-2.5-pro\",\n    name: \"Gemini 2.5 Pro\",\n    provider: \"google\",\n    contextWindow: 1000000,\n    maxOutputTokens: 64000,\n    capabilities: { streaming: true, functionCalling: true, vision: true, reasoning: true },\n    pricing: { input: 1.25, output: 5.0 },\n  },\n  \"gemini-2.5-flash\": {\n    id: \"gemini-2.5-flash\",\n    name: \"Gemini 2.5 Flash\",\n    provider: \"google\",\n    contextWindow: 1000000,\n    maxOutputTokens: 64000,\n    capabilities: { streaming: true, functionCalling: true, vision: true },\n    pricing: { input: 0.075, output: 0.3 },\n  },\n\n  // xAI\n  \"grok-4\": {\n    id: \"grok-4\",\n    name: \"Grok 4\",\n    provider: \"xai\",\n    contextWindow: 200000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true, reasoning: true },\n    pricing: { input: 5.0, output: 15.0 },\n  },\n  \"grok-3-fast\": {\n    id: \"grok-3-fast\",\n    name: \"Grok 3 Fast\",\n    provider: \"xai\",\n    contextWindow: 131072,\n    maxOutputTokens: 32768,\n    capabilities: { streaming: true, functionCalling: true },\n    pricing: { input: 3.0, output: 15.0 },\n  },\n  \"grok-3\": {\n    id: \"grok-3\",\n    name: \"Grok 3\",\n    provider: \"xai\",\n    contextWindow: 131072,\n    maxOutputTokens: 32768,\n    capabilities: { streaming: true, functionCalling: true },\n    pricing: { input: 5.0, output: 15.0 },\n  },\n\n  // DeepSeek\n  \"deepseek-v3\": {\n    id: \"deepseek-v3\",\n    name: \"DeepSeek V3\",\n    provider: \"deepseek\",\n    contextWindow: 64000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true },\n    pricing: { input: 0.27, output: 1.1 },\n  },\n  \"deepseek-chat\": {\n    id: \"deepseek-chat\",\n    name: \"DeepSeek Chat\",\n    provider: \"deepseek\",\n    contextWindow: 64000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true },\n    pricing: { input: 0.14, output: 0.28 },\n  },\n\n  // Moonshot (Kimi)\n  \"kimi-k2.5\": {\n    id: \"kimi-k2.5\",\n    name: \"Kimi K2.5\",\n    provider: \"moonshot\",\n    contextWindow: 128000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true, vision: true },\n    pricing: { input: 0.6, output: 2.0 },\n  },\n  \"kimi-k2.5-mini\": {\n    id: \"kimi-k2.5-mini\",\n    name: \"Kimi K2.5 Mini\",\n    provider: \"moonshot\",\n    contextWindow: 128000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true, vision: true },\n    pricing: { input: 0.2, output: 0.6 },\n  },\n\n  // OpenCode\n  \"big-pickle\": {\n    id: \"big-pickle\",\n    name: \"Big Pickle\",\n    provider: \"opencode\",\n    contextWindow: 200000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true, reasoning: true },\n    pricing: { input: 0.0, output: 0.0 },\n  },\n  \"gpt-5-nano\": {\n    id: \"gpt-5-nano\",\n    name: \"GPT-5 Nano\",\n    provider: \"opencode\",\n    contextWindow: 128000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true },\n    pricing: { input: 0.0, output: 0.0 },\n  },\n  \"kimi-k2.5-free\": {\n    id: \"kimi-k2.5-free\",\n    name: \"Kimi K2.5 Free\",\n    provider: \"opencode\",\n    contextWindow: 128000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true },\n    pricing: { input: 0.0, output: 0.0 },\n  },\n  \"minimax-m2.5-free\": {\n    id: \"minimax-m2.5-free\",\n    name: \"MiniMax M2.5 Free\",\n    provider: \"opencode\",\n    contextWindow: 128000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true },\n    pricing: { input: 0.0, output: 0.0 },\n  },\n\n  // OpenRouter (meta-provider)\n  \"openrouter/auto\": {\n    id: \"openrouter/auto\",\n    name: \"OpenRouter Auto\",\n    provider: \"openrouter\",\n    contextWindow: 128000,\n    maxOutputTokens: 32000,\n    capabilities: { streaming: true, functionCalling: true },\n  },\n  \n  // NVIDIA\n  \"z-ai/glm5\": {\n    id: \"z-ai/glm5\",\n    name: \"GLM-5\",\n    provider: \"nvidia\",\n    contextWindow: 128000,\n    maxOutputTokens: 4096,\n    capabilities: { streaming: true, functionCalling: true },\n    pricing: { input: 0.0, output: 0.0 },\n  },\n  \"nvidia/llama-3.1-nemotron-70b-instruct\": {\n    id: \"nvidia/llama-3.1-nemotron-70b-instruct\",\n    name: \"Nemotron 70B\",\n    provider: \"nvidia\",\n    contextWindow: 128000,\n    maxOutputTokens: 4096,\n    capabilities: { streaming: true, functionCalling: true },\n    pricing: { input: 0.0, output: 0.0 },\n  },\n  \"nvidia/mistral-large\": {\n    id: \"nvidia/mistral-large\",\n    name: \"Mistral Large (NVIDIA)\",\n    provider: \"nvidia\",\n    contextWindow: 128000,\n    maxOutputTokens: 4096,\n    capabilities: { streaming: true, functionCalling: true },\n    pricing: { input: 0.0, output: 0.0 },\n  },\n};\n\n// Model aliases for convenient access\nexport const MODEL_ALIASES: ModelAlias[] = [\n  { alias: \"auto\", model: \"omni-llm/auto\" },\n  { alias: \"sonnet\", model: \"claude-sonnet-4\" },\n  { alias: \"opus\", model: \"claude-opus-4\" },\n  { alias: \"haiku\", model: \"claude-haiku-4\" },\n  { alias: \"gpt\", model: \"gpt-4o\" },\n  { alias: \"gpt-mini\", model: \"gpt-4o-mini\" },\n  { alias: \"flash\", model: \"gemini-2.5-flash\" },\n  { alias: \"pro\", model: \"gemini-2.5-pro\" },\n  { alias: \"deepseek\", model: \"deepseek-v3\" },\n  { alias: \"grok\", model: \"grok-3\" },\n  { alias: \"grok-reasoning\", model: \"grok-3-fast\" },\n  { alias: \"kimi\", model: \"kimi-k2.5\" },\n  { alias: \"big-pickle\", model: \"big-pickle\", provider: \"opencode\" },\n  { alias: \"gpt-nano\", model: \"gpt-5-nano\", provider: \"opencode\" },\n  { alias: \"reasoner\", model: \"o3-mini\" },\n  { alias: \"reasoning\", model: \"o3\" },\n  { alias: \"glm5\", model: \"z-ai/glm5\" },\n  { alias: \"nemotron\", model: \"nvidia/llama-3.1-nemotron-70b-instruct\" },\n];\n\n// Convert internal model to OpenClaw ModelDefinitionConfig format\nfunction toOpenClawModel(m: ModelInfo): ModelDefinitionConfig {\n  return {\n    id: m.id,\n    name: m.name,\n    api: \"openai-completions\",\n    reasoning: m.capabilities?.reasoning ?? false,\n    input: m.capabilities?.vision ? [\"text\", \"image\"] : [\"text\"],\n    cost: {\n      input: m.pricing?.input ?? 0,\n      output: m.pricing?.output ?? 0,\n      cacheRead: 0,\n      cacheWrite: 0,\n    },\n    contextWindow: m.contextWindow,\n    maxTokens: m.maxOutputTokens,\n  };\n}\n\n// Create alias models for friendly names\nconst ALIAS_MODELS: ModelDefinitionConfig[] = MODEL_ALIASES\n  .map((alias) => {\n    const target = MODELS[alias.model.replace(\"omni-llm/\", \"\")];\n    if (!target) return null;\n    return toOpenClawModel({ ...target, id: alias.alias, name: `${alias.alias} â†’ ${target.name}` });\n  })\n  .filter((m): m is ModelDefinitionConfig => m !== null);\n\n// All models in OpenClaw format\nexport const OPENCLAW_MODELS: ModelDefinitionConfig[] = [\n  // Smart routing meta-model\n  {\n    id: \"auto\",\n    name: \"OmniLLM Smart Router\",\n    api: \"openai-completions\",\n    reasoning: false,\n    input: [\"text\"],\n    cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },\n    contextWindow: 200000,\n    maxTokens: 64000,\n  },\n  // Convert all regular models\n  ...Object.values(MODELS).map(toOpenClawModel),\n  // Add aliases\n  ...ALIAS_MODELS,\n];\n\n// Legacy model list for backward compatibility\nexport const OPENCLAW_MODEL_LIST = [\n  \"omni-llm/auto\",\n  \"omni-llm/sonnet\",\n  \"omni-llm/opus\",\n  \"omni-llm/haiku\",\n  \"omni-llm/gpt\",\n  \"omni-llm/gpt-mini\",\n  \"omni-llm/flash\",\n  \"omni-llm/pro\",\n  \"omni-llm/deepseek\",\n  \"omni-llm/grok\",\n  \"omni-llm/grok-reasoning\",\n  \"omni-llm/kimi\",\n  \"omni-llm/big-pickle\",\n  \"omni-llm/gpt-nano\",\n  \"omni-llm/reasoner\",\n  \"omni-llm/reasoning\",\n];\n\n/**\n * Build a ModelProviderConfig for OmniLLM.\n * \n * @param baseUrl - The proxy's local base URL (e.g., \"http://127.0.0.1:8403\")\n */\nexport function buildProviderModels(baseUrl: string): ModelProviderConfig {\n  return {\n    baseUrl: `${baseUrl}/v1`,\n    api: \"openai-completions\",\n    apiKey: \"local-proxy\",\n    models: OPENCLAW_MODELS,\n  };\n}\n\n/**\n * Get model info by ID\n */\nexport function getModel(modelId: string): ModelInfo | undefined {\n  return MODELS[modelId];\n}\n\n/**\n * Resolve model alias\n */\nexport function resolveModelAlias(alias: string): string {\n  const found = MODEL_ALIASES.find((a) => a.alias === alias);\n  return found?.model || alias;\n}\n\n/**\n * Get models by provider\n */\nexport function getModelsByProvider(provider: LLMProvider): ModelInfo[] {\n  return Object.values(MODELS).filter((m) => m.provider === provider);\n}\n\n/**\n * Get free models (no API cost)\n */\nexport function getFreeModels(): ModelInfo[] {\n  return Object.values(MODELS).filter((m) => m.pricing?.input === 0);\n}\n\n/**\n * Check if model is optimized for agentic workflows\n */\nexport function isAgenticModel(modelId: string): boolean {\n  const model = MODELS[modelId];\n  return model?.capabilities?.functionCalling ?? false;\n}\n\n/**\n * Get all agentic-capable models\n */\nexport function getAgenticModels(): string[] {\n  return Object.values(MODELS)\n    .filter((m) => m.capabilities?.functionCalling)\n    .map((m) => m.id);\n}\n\n/**\n * Get context window size for a model\n */\nexport function getModelContextWindow(modelId: string): number | undefined {\n  return MODELS[modelId]?.contextWindow;\n}\n","/**\n * OmniLLM ProviderPlugin for OpenClaw\n * \n * Registers OmniLLM as an LLM provider in OpenClaw.\n * Uses a local proxy to handle smart routing to provider APIs.\n */\n\nimport type { ProviderPlugin } from \"./types/openclaw.js\";\nimport { buildProviderModels } from \"./models.js\";\n\n// Default proxy port\nconst DEFAULT_PORT = 8403;\n\nexport function getProxyPort(): number {\n  const envPort = process.env.OMNI_LLM_PORT;\n  if (envPort) {\n    const parsed = parseInt(envPort, 10);\n    if (!isNaN(parsed) && parsed > 0 && parsed < 65536) return parsed;\n  }\n  return DEFAULT_PORT;\n}\n\nexport const omniLLMProvider: ProviderPlugin = {\n  id: \"omni-llm\",\n  label: \"OmniLLM\",\n  docsPath: \"https://github.com/omni-llm/omni-provider\",\n  aliases: [\"omni\"],\n  envVars: [\n    \"OPENROUTER_API_KEY\",\n    \"OPENAI_API_KEY\",\n    \"ANTHROPIC_API_KEY\",\n    \"GOOGLE_API_KEY\",\n    \"XAI_API_KEY\",\n    \"DEEPSEEK_API_KEY\",\n    \"MOONSHOT_API_KEY\",\n    \"OPENCODE_API_KEY\",\n  ],\n\n  get models() {\n    const port = getProxyPort();\n    return buildProviderModels(`http://127.0.0.1:${port}`);\n  },\n\n  auth: [],\n};\n","export const VERSION = '1.0.0';\n","/**\n * Configuration Loader\n * \n * Loads provider configuration from plugin config and environment variables.\n */\n\nimport type { ProvidersConfig, ProviderConfig, LLMProvider } from \"../types.js\";\n\n/**\n * Validate provider configuration\n */\nexport function validateConfig(config: ProvidersConfig): { valid: boolean; errors: string[] } {\n  const errors: string[] = [];\n  \n  for (const [provider, cfg] of Object.entries(config)) {\n    if (cfg.enabled && !cfg.apiKey) {\n      errors.push(`Provider ${provider} is enabled but missing API key`);\n    }\n  }\n  \n  return {\n    valid: errors.length === 0,\n    errors,\n  };\n}\n\n/**\n * Merge two provider configs\n */\nexport function mergeConfigs(base: ProvidersConfig, override: ProvidersConfig): ProvidersConfig {\n  return {\n    ...base,\n    ...override,\n  };\n}\n\n/**\n * Load config from env vars (test helper)\n */\nexport function loadConfig(env: Record<string, string | undefined>): ProvidersConfig {\n  const originalEnv = process.env;\n  // @ts-ignore - temporarily replace env\n  process.env = { ...originalEnv, ...env };\n  \n  const defaultConfig: ProvidersConfig = {\n    openrouter: { enabled: false },\n    openai: { enabled: false },\n    anthropic: { enabled: false },\n    google: { enabled: false },\n    xai: { enabled: false },\n    deepseek: { enabled: false },\n    moonshot: { enabled: false },\n    opencode: { enabled: false },\n    azure: { enabled: false },\n    anyscale: { enabled: false },\n    together: { enabled: false },\n    fireworks: { enabled: false },\n    mistral: { enabled: false },\n    cohere: { enabled: false },\n    perplexity: { enabled: false },\n    chutes: { enabled: false },\n  };\n  \n  const config = applyEnvOverrides(defaultConfig);\n  \n  // Restore original env\n  process.env = originalEnv;\n  \n  return config;\n}\n\nexport interface ApiKeysConfig {\n  openrouter?: { apiKey?: string };\n  openai?: { apiKey?: string };\n  anthropic?: { apiKey?: string };\n  google?: { apiKey?: string };\n  xai?: { apiKey?: string };\n  deepseek?: { apiKey?: string };\n  moonshot?: { apiKey?: string };\n  opencode?: { apiKey?: string };\n  azure?: { apiKey?: string; endpoint?: string };\n  anyscale?: { apiKey?: string };\n  together?: { apiKey?: string };\n  fireworks?: { apiKey?: string };\n  mistral?: { apiKey?: string };\n  cohere?: { apiKey?: string };\n  perplexity?: { apiKey?: string };\n  [key: string]: unknown;\n}\n\n/**\n * Load provider configuration from plugin config\n */\nexport function loadProviderConfig(pluginConfig?: Record<string, unknown>): { providers: ProvidersConfig; routing?: Record<string, unknown> } {\n  const defaultConfig: ProvidersConfig = {\n    openrouter: { enabled: false },\n    openai: { enabled: false },\n    anthropic: { enabled: false },\n    google: { enabled: false },\n    xai: { enabled: false },\n    deepseek: { enabled: false },\n    moonshot: { enabled: false },\n    opencode: { enabled: false },\n    azure: { enabled: false },\n    anyscale: { enabled: false },\n    together: { enabled: false },\n    fireworks: { enabled: false },\n    mistral: { enabled: false },\n    cohere: { enabled: false },\n    perplexity: { enabled: false },\n    chutes: { enabled: false },\n  };\n\n  let config: ProvidersConfig = { ...defaultConfig };\n\n  // Override with plugin config\n  if (pluginConfig?.providers) {\n    const providers = pluginConfig.providers as Record<string, ProviderConfig>;\n    for (const [provider, cfg] of Object.entries(providers)) {\n      if (isValidProvider(provider)) {\n        config[provider] = { ...defaultConfig[provider], ...cfg };\n      }\n    }\n  }\n\n  // Override with environment variables\n  config = applyEnvOverrides(config);\n\n  return { providers: config, routing: pluginConfig?.routing as Record<string, unknown> | undefined };\n}\n\n/**\n * Apply environment variable overrides\n */\nfunction applyEnvOverrides(config: ProvidersConfig): ProvidersConfig {\n  // OpenRouter\n  if (process.env.OPENROUTER_API_KEY) {\n    config.openrouter = { ...config.openrouter, enabled: true, apiKey: process.env.OPENROUTER_API_KEY };\n  }\n\n  // OpenAI\n  if (process.env.OPENAI_API_KEY) {\n    config.openai = { ...config.openai, enabled: true, apiKey: process.env.OPENAI_API_KEY };\n  }\n\n  // Anthropic\n  if (process.env.ANTHROPIC_API_KEY) {\n    config.anthropic = { ...config.anthropic, enabled: true, apiKey: process.env.ANTHROPIC_API_KEY };\n  }\n\n  // Google\n  if (process.env.GOOGLE_API_KEY) {\n    config.google = { ...config.google, enabled: true, apiKey: process.env.GOOGLE_API_KEY };\n  }\n\n  // xAI\n  if (process.env.XAI_API_KEY) {\n    config.xai = { ...config.xai, enabled: true, apiKey: process.env.XAI_API_KEY };\n  }\n\n  // DeepSeek\n  if (process.env.DEEPSEEK_API_KEY) {\n    config.deepseek = { ...config.deepseek, enabled: true, apiKey: process.env.DEEPSEEK_API_KEY };\n  }\n\n  // Moonshot\n  if (process.env.MOONSHOT_API_KEY) {\n    config.moonshot = { ...config.moonshot, enabled: true, apiKey: process.env.MOONSHOT_API_KEY };\n  }\n\n  // OpenCode\n  if (process.env.OPENCODE_API_KEY) {\n    config.opencode = { ...config.opencode, enabled: true, apiKey: process.env.OPENCODE_API_KEY };\n  }\n\n  // Azure\n  if (process.env.AZURE_OPENAI_API_KEY && process.env.AZURE_OPENAI_ENDPOINT) {\n    config.azure = { \n      ...config.azure, \n      enabled: true, \n      apiKey: process.env.AZURE_OPENAI_API_KEY,\n      baseUrl: process.env.AZURE_OPENAI_ENDPOINT \n    };\n  }\n\n  // Anyscale\n  if (process.env.ANYSCALE_API_KEY) {\n    config.anyscale = { ...config.anyscale, enabled: true, apiKey: process.env.ANYSCALE_API_KEY };\n  }\n\n  // Together\n  if (process.env.TOGETHER_API_KEY) {\n    config.together = { ...config.together, enabled: true, apiKey: process.env.TOGETHER_API_KEY };\n  }\n\n  // Fireworks\n  if (process.env.FIREWORKS_API_KEY) {\n    config.fireworks = { ...config.fireworks, enabled: true, apiKey: process.env.FIREWORKS_API_KEY };\n  }\n\n  // Mistral\n  if (process.env.MISTRAL_API_KEY) {\n    config.mistral = { ...config.mistral, enabled: true, apiKey: process.env.MISTRAL_API_KEY };\n  }\n\n  // Cohere\n  if (process.env.COHERE_API_KEY) {\n    config.cohere = { ...config.cohere, enabled: true, apiKey: process.env.COHERE_API_KEY };\n  }\n\n  // Perplexity\n  if (process.env.PERPLEXITY_API_KEY) {\n    config.perplexity = { ...config.perplexity, enabled: true, apiKey: process.env.PERPLEXITY_API_KEY };\n  }\n\n  // Chutes\n  if (process.env.CHUTES_API_KEY) {\n    config.chutes = { ...config.chutes, enabled: true, apiKey: process.env.CHUTES_API_KEY };\n  }\n\n  return config;\n}\n\n/**\n * Check if provider is valid\n */\nfunction isValidProvider(provider: string): provider is LLMProvider {\n  const validProviders: LLMProvider[] = [\n    'openrouter', 'openai', 'anthropic', 'google', 'xai', \n    'deepseek', 'moonshot', 'opencode', 'azure', 'anyscale',\n    'together', 'fireworks', 'mistral', 'cohere', 'perplexity',\n    'chutes'\n  ];\n  return validProviders.includes(provider as LLMProvider);\n}\n\n/**\n * Load API keys from environment (for runtime)\n */\nexport function loadApiKeysFromEnv(): ApiKeysConfig {\n  return {\n    openrouter: { apiKey: process.env.OPENROUTER_API_KEY },\n    openai: { apiKey: process.env.OPENAI_API_KEY },\n    anthropic: { apiKey: process.env.ANTHROPIC_API_KEY },\n    google: { apiKey: process.env.GOOGLE_API_KEY },\n    xai: { apiKey: process.env.XAI_API_KEY },\n    deepseek: { apiKey: process.env.DEEPSEEK_API_KEY },\n    moonshot: { apiKey: process.env.MOONSHOT_API_KEY },\n    opencode: { apiKey: process.env.OPENCODE_API_KEY },\n    azure: { apiKey: process.env.AZURE_OPENAI_API_KEY, endpoint: process.env.AZURE_OPENAI_ENDPOINT },\n    anyscale: { apiKey: process.env.ANYSCALE_API_KEY },\n    together: { apiKey: process.env.TOGETHER_API_KEY },\n    fireworks: { apiKey: process.env.FIREWORKS_API_KEY },\n    mistral: { apiKey: process.env.MISTRAL_API_KEY },\n    cohere: { apiKey: process.env.COHERE_API_KEY },\n    perplexity: { apiKey: process.env.PERPLEXITY_API_KEY },\n    chutes: { apiKey: process.env.CHUTES_API_KEY },\n  };\n}\n\n/**\n * Get environment variable usage instructions\n */\nexport function getEnvInstructions(): string {\n  return `\nEnvironment Variables (LLM Providers):\n  OPENROUTER_API_KEY    - OpenRouter (all models)\n  OPENAI_API_KEY        - OpenAI (GPT-4o, GPT-5, o-series)\n  ANTHROPIC_API_KEY     - Anthropic (Claude)\n  GOOGLE_API_KEY        - Google (Gemini)\n  XAI_API_KEY           - xAI (Grok)\n  DEEPSEEK_API_KEY      - DeepSeek\n  MOONSHOT_API_KEY      - Moonshot (Kimi)\n  OPENCODE_API_KEY      - OpenCode models\n  AZURE_OPENAI_API_KEY  - Azure OpenAI\n  ANYSCALE_API_KEY      - Anyscale\n  TOGETHER_API_KEY      - Together AI\n  FIREWORKS_API_KEY     - Fireworks AI\n  MISTRAL_API_KEY       - Mistral\n  COHERE_API_KEY        - Cohere\n  PERPLEXITY_API_KEY    - Perplexity\n  CHUTES_API_KEY        - Chutes (cheap/discounted models)\n`.trim();\n}\n","/**\n * Stats Collector and Utilities\n */\n\nimport type { LLMProvider, ProviderStats, AggregatedStats } from \"./types\";\n\nexport interface DailyStats {\n  date: string;\n  requests: number;\n  promptTokens: number;\n  completionTokens: number;\n  cost: number;\n}\n\nexport class StatsCollector {\n  private stats: Map<LLMProvider, ProviderStats> = new Map();\n\n  recordRequest(provider: LLMProvider, promptTokens: number, completionTokens: number, latencyMs: number): void {\n    let stat = this.stats.get(provider);\n    if (!stat) {\n      stat = { provider, requests: 0, promptTokens: 0, completionTokens: 0, totalCost: 0, errors: 0, avgLatency: 0, lastActivity: 0 };\n      this.stats.set(provider, stat);\n    }\n    stat.requests++;\n    stat.promptTokens += promptTokens;\n    stat.completionTokens += completionTokens;\n    stat.avgLatency = (stat.avgLatency * (stat.requests - 1) + latencyMs) / stat.requests;\n    stat.lastActivity = Date.now();\n  }\n\n  recordError(provider: LLMProvider): void {\n    const stat = this.stats.get(provider);\n    if (stat) stat.errors++;\n  }\n\n  getAggregated(): AggregatedStats {\n    let totalRequests = 0, totalPrompt = 0, totalCompletion = 0, totalCost = 0, totalErrors = 0;\n    for (const stat of this.stats.values()) {\n      totalRequests += stat.requests;\n      totalPrompt += stat.promptTokens;\n      totalCompletion += stat.completionTokens;\n      totalErrors += stat.errors;\n    }\n    return { totalRequests, totalPromptTokens: totalPrompt, totalCompletionTokens: totalCompletion, totalCost, totalErrors, byProvider: Array.from(this.stats.values()), savingsVsOpenRouter: 0 };\n  }\n}\n\n/**\n * Get stats for a time period\n */\nexport async function getStats(days: number): Promise<DailyStats[]> {\n  // Placeholder implementation - in production this would read from logs\n  return Array.from({ length: days }, (_, i) => ({\n    date: new Date(Date.now() - i * 24 * 60 * 60 * 1000).toISOString().split(\"T\")[0],\n    requests: 0,\n    promptTokens: 0,\n    completionTokens: 0,\n    cost: 0,\n  }));\n}\n\n/**\n * Format stats as ASCII table\n */\nexport function formatStatsAscii(stats: DailyStats[]): string {\n  if (stats.length === 0) return \"No stats available\";\n  \n  const lines = [\n    \"OmniLLM Usage Statistics\",\n    \"========================\",\n    \"\",\n    \"Date        | Requests | Tokens  | Cost\",\n    \"------------|----------|---------|-------\",\n  ];\n  \n  for (const day of stats) {\n    const tokens = day.promptTokens + day.completionTokens;\n    lines.push(\n      `${day.date} | ${day.requests.toString().padStart(8)} | ${tokens.toString().padStart(7)} | $${day.cost.toFixed(4)}`\n    );\n  }\n  \n  return lines.join(\"\\n\");\n}\n","/**\n * OmniLLM Proxy Server\n * \n * Local HTTP server that routes OpenAI-compatible requests to multiple LLM providers.\n * Runs on port 8403 by default.\n */\n\nimport { createServer, type IncomingMessage, type ServerResponse } from \"node:http\";\nimport type { AddressInfo } from \"node:net\";\nimport { loadApiKeysFromEnv, type ApiKeysConfig } from \"./config/index.js\";\nimport { resolveModelAlias, getModel } from \"./models.js\";\nimport type { ModelInfo } from \"./types.js\";\nimport type { LLMProvider } from \"./types.js\";\n\nconst DEFAULT_PORT = 8403;\nconst HEALTH_CHECK_TIMEOUT_MS = 2000;\n\n// Provider base URLs\nconst PROVIDER_URLS: Record<LLMProvider, string> = {\n  openrouter: \"https://openrouter.ai/api/v1\",\n  openai: \"https://api.openai.com/v1\",\n  anthropic: \"https://api.anthropic.com/v1\",\n  google: \"https://generativelanguage.googleapis.com/v1beta\",\n  xai: \"https://api.x.ai/v1\",\n  deepseek: \"https://api.deepseek.com/v1\",\n  moonshot: \"https://api.moonshot.cn/v1\",\n  opencode: \"https://opencode.ai/zen/v1\",\n  azure: \"\",\n  anyscale: \"https://api.endpoints.anyscale.com/v1\",\n  together: \"https://api.together.xyz/v1\",\n  fireworks: \"https://api.fireworks.ai/inference/v1\",\n  mistral: \"https://api.mistral.ai/v1\",\n  cohere: \"https://api.cohere.ai/v1\",\n  perplexity: \"https://api.perplexity.ai\",\n  nvidia: \"https://integrate.api.nvidia.com/v1\",\n  chutes: \"https://llm.chutes.ai/v1\",\n};\n\n// Map provider to environment variable name\nconst PROVIDER_ENV_VARS: Record<LLMProvider, string> = {\n  openrouter: \"OPENROUTER_API_KEY\",\n  openai: \"OPENAI_API_KEY\",\n  anthropic: \"ANTHROPIC_API_KEY\",\n  google: \"GOOGLE_API_KEY\",\n  xai: \"XAI_API_KEY\",\n  deepseek: \"DEEPSEEK_API_KEY\",\n  moonshot: \"MOONSHOT_API_KEY\",\n  opencode: \"OPENCODE_API_KEY\",\n  azure: \"AZURE_OPENAI_API_KEY\",\n  anyscale: \"ANYSCALE_API_KEY\",\n  together: \"TOGETHER_API_KEY\",\n  fireworks: \"FIREWORKS_API_KEY\",\n  mistral: \"MISTRAL_API_KEY\",\n  cohere: \"COHERE_API_KEY\",\n  perplexity: \"PERPLEXITY_API_KEY\",\n  nvidia: \"NVIDIA_API_KEY\",\n  chutes: \"CHUTES_API_KEY\",\n};\n\nexport interface ProxyHandle {\n  port: number;\n  baseUrl: string;\n  close: () => Promise<void>;\n}\n\ninterface ProxyOptions {\n  port?: number;\n  onReady?: (port: number) => void;\n  onError?: (error: Error) => void;\n  onRequest?: (request: unknown) => void;\n}\n\n/**\n * Get proxy port from environment or default\n */\nexport function getProxyPort(): number {\n  const envPort = process.env.OMNI_LLM_PORT;\n  if (envPort) {\n    const parsed = parseInt(envPort, 10);\n    if (!isNaN(parsed) && parsed > 0 && parsed < 65536) return parsed;\n  }\n  return DEFAULT_PORT;\n}\n\n/**\n * Check if proxy is already running on port\n */\nasync function checkExistingProxy(port: number): Promise<boolean> {\n  const controller = new AbortController();\n  const timeoutId = setTimeout(() => controller.abort(), HEALTH_CHECK_TIMEOUT_MS);\n  try {\n    const response = await fetch(`http://127.0.0.1:${port}/health`, { \n      signal: controller.signal \n    });\n    clearTimeout(timeoutId);\n    if (response.ok) {\n      const data = (await response.json()) as { status?: string };\n      return data.status === \"ok\";\n    }\n    return false;\n  } catch {\n    clearTimeout(timeoutId);\n    return false;\n  }\n}\n\n/**\n * Classify request based on content\n */\nfunction classifyRequest(request: any): 'simple' | 'medium' | 'complex' | 'reasoning' | 'vision' {\n  const content = request.messages?.map((m: any) => m.content).join(' ').toLowerCase() || '';\n  \n  // Check for reasoning keywords\n  if (/prove|explain.*step|calculate|derive|logic|proof|theorem|math|code|debug|analyze|complex/i.test(content)) {\n    return 'reasoning';\n  }\n  \n  // Check for vision (has images in messages)\n  if (request.messages?.some((m: any) => typeof m.content === 'object' && Array.isArray(m.content) && m.content.some((p: any) => p.type === 'image_url'))) {\n    return 'vision';\n  }\n\n  // Check complexity by length\n  if (content.length < 200) return 'simple';\n  if (content.length < 2000) return 'medium';\n  return 'complex';\n}\n\n/**\n * Get model for tier and available providers\n * Strategy: \n * - Simple: OpenCode free models\n * - Medium/High: Chutes GLM5, Kimi 2.5 thinking, R1T2-Chimera-Speed\n * - Highest complexity: OpenRouter Opus 4.6\n */\nfunction getModelForTier(tier: string): { provider: LLMProvider; modelId: string; isPaid: boolean } | null {\n  // Define model preferences per tier\n  const tierModels: Record<string, Array<{ provider: LLMProvider; model: string; isPaid: boolean }>> = {\n    simple: [\n      // Rotate between OpenCode Big Pickle and OpenRouter free auto router\n      { provider: 'opencode', model: 'big-pickle', isPaid: false },\n      { provider: 'openrouter', model: 'openrouter/auto', isPaid: false },\n      // Other OpenCode free models\n      { provider: 'opencode', model: 'kimi-k2.5-free', isPaid: false },\n      { provider: 'opencode', model: 'gpt-5-nano', isPaid: false },\n      { provider: 'opencode', model: 'minimax-m2.5-free', isPaid: false },\n      // Additional fallbacks\n      { provider: 'chutes', model: 'chutes/llama-3.1-70b', isPaid: false },\n      { provider: 'nvidia', model: 'nvidia/llama-3.1-nemotron-70b-instruct', isPaid: false },\n      { provider: 'nvidia', model: 'nvidia/mistral-large', isPaid: false },\n    ],\n    medium: [\n      // Chutes models for medium complexity (excellent performance, good limits)\n      { provider: 'chutes', model: 'z-ai/glm5', isPaid: false },\n      { provider: 'chutes', model: 'z-ai/kimi-k2.5', isPaid: false },\n      { provider: 'chutes', model: 'chutes/R1T2-Chimera-Speed', isPaid: false },\n      // Alternative GLM5 options (OpenRouter has better limits than NVIDIA)\n      { provider: 'openrouter', model: 'z-ai/glm-4.5', isPaid: false },\n      { provider: 'openrouter', model: 'z-ai/glm-4.5-air', isPaid: false },\n      // NVIDIA - NOTE: GLM5 heavily rate-limited on NVIDIA, use other models\n      { provider: 'nvidia', model: 'nvidia/llama-3.1-nemotron-70b-instruct', isPaid: false },\n      { provider: 'nvidia', model: 'nvidia/mistral-large', isPaid: false },\n      // Fallback to OpenCode\n      { provider: 'opencode', model: 'big-pickle', isPaid: false },\n      { provider: 'opencode', model: 'gpt-5-nano', isPaid: false },\n    ],\n    complex: [\n      // Chutes high-performance models for high complexity\n      { provider: 'chutes', model: 'z-ai/glm5', isPaid: false },\n      { provider: 'chutes', model: 'z-ai/kimi-k2.5', isPaid: false },\n      { provider: 'chutes', model: 'chutes/R1T2-Chimera-Speed', isPaid: false },\n      // OpenRouter mid-tier as fallback\n      { provider: 'openrouter', model: 'anthropic/claude-sonnet-4', isPaid: true },\n      { provider: 'openrouter', model: 'google/gemini-2.5-pro', isPaid: true },\n      { provider: 'openrouter', model: 'openai/gpt-4o', isPaid: true },\n    ],\n    reasoning: [\n      // Chutes reasoning models\n      { provider: 'chutes', model: 'z-ai/kimi-k2.5', isPaid: false },\n      { provider: 'chutes', model: 'chutes/R1T2-Chimera-Speed', isPaid: false },\n      // OpenRouter reasoning models\n      { provider: 'openrouter', model: 'deepseek/deepseek-r1', isPaid: true },\n      { provider: 'openrouter', model: 'openai/o3-mini', isPaid: true },\n      // HIGHEST COMPLEXITY - Opus 4.6 via OpenRouter\n      { provider: 'openrouter', model: 'anthropic/claude-opus-4', isPaid: true },\n    ],\n    vision: [\n      // Free vision models\n      { provider: 'opencode', model: 'big-pickle', isPaid: false },\n      { provider: 'chutes', model: 'z-ai/glm5', isPaid: false },\n      // OpenRouter vision models\n      { provider: 'openrouter', model: 'google/gemini-2.5-flash', isPaid: false },\n      { provider: 'openrouter', model: 'anthropic/claude-sonnet-4', isPaid: true },\n      { provider: 'openrouter', model: 'openai/gpt-4o', isPaid: true },\n    ],\n  };\n\n  const models = tierModels[tier] || tierModels.medium;\n  \n  // Find first available provider with API key\n  for (const { provider, model, isPaid } of models) {\n    const apiKey = getApiKey(provider);\n    if (apiKey) {\n      console.log(`[OmniLLM] Selected ${isPaid ? 'PAID' : 'FREE'} model: ${provider}/${model} for ${tier} tier`);\n      return { provider, modelId: model, isPaid };\n    }\n  }\n  \n  // Fallback to any available provider\n  const fallback = getFirstAvailableProvider();\n  if (fallback) {\n    return { ...fallback, isPaid: false };\n  }\n  \n  return null;\n}\n\n/**\n * Get ALL models for a tier with available API keys (for rotation)\n */\nfunction getAllModelsForTier(tier: string): Array<{ provider: LLMProvider; modelId: string; isPaid: boolean }> {\n  // Define model preferences per tier\n  const tierModels: Record<string, Array<{ provider: LLMProvider; model: string; isPaid: boolean }>> = {\n    simple: [\n      // Rotate between OpenCode Big Pickle and OpenRouter free auto router\n      { provider: 'opencode', model: 'big-pickle', isPaid: false },\n      { provider: 'openrouter', model: 'openrouter/auto', isPaid: false },\n      // Other OpenCode free models\n      { provider: 'opencode', model: 'kimi-k2.5-free', isPaid: false },\n      { provider: 'opencode', model: 'gpt-5-nano', isPaid: false },\n      { provider: 'opencode', model: 'minimax-m2.5-free', isPaid: false },\n      // Additional fallbacks\n      { provider: 'chutes', model: 'chutes/llama-3.1-70b', isPaid: false },\n      { provider: 'nvidia', model: 'nvidia/llama-3.1-nemotron-70b-instruct', isPaid: false },\n      { provider: 'nvidia', model: 'nvidia/mistral-large', isPaid: false },\n    ],\n    medium: [\n      // Chutes GLM and Kimi first\n      { provider: 'chutes', model: 'zai-org/GLM-5-TEE', isPaid: false },\n      { provider: 'chutes', model: 'moonshotai/Kimi-K2.5-TEE', isPaid: false },\n      // OpenCode as backup\n      { provider: 'opencode', model: 'gpt-5-nano', isPaid: false },\n      // OpenRouter GLM\n      { provider: 'openrouter', model: 'z-ai/glm-4.5', isPaid: false },\n      { provider: 'openrouter', model: 'z-ai/glm-4.5-air', isPaid: false },\n      // NVIDIA\n      { provider: 'nvidia', model: 'nvidia/llama-3.1-nemotron-70b-instruct', isPaid: false },\n      { provider: 'nvidia', model: 'nvidia/mistral-large', isPaid: false },\n    ],\n    complex: [\n      // Chutes GLM and Kimi\n      { provider: 'chutes', model: 'zai-org/GLM-5-TEE', isPaid: false },\n      { provider: 'chutes', model: 'moonshotai/Kimi-K2.5-TEE', isPaid: false },\n      // OpenRouter\n      { provider: 'openrouter', model: 'z-ai/glm-4.5', isPaid: false },\n      // NVIDIA\n      { provider: 'nvidia', model: 'nvidia/mistral-large', isPaid: false },\n      // Paid fallbacks\n      { provider: 'openrouter', model: 'anthropic/claude-sonnet-4', isPaid: true },\n      { provider: 'openrouter', model: 'google/gemini-2.5-pro', isPaid: true },\n      { provider: 'openrouter', model: 'openai/gpt-4o', isPaid: true },\n    ],\n    reasoning: [\n      // Chutes Kimi for reasoning\n      { provider: 'chutes', model: 'moonshotai/Kimi-K2.5-TEE', isPaid: false },\n      // OpenRouter o3-mini for reasoning\n      { provider: 'openrouter', model: 'openai/o3-mini', isPaid: true },\n      // HIGHEST COMPLEXITY - Opus 4.6 via OpenRouter\n      { provider: 'openrouter', model: 'anthropic/claude-opus-4', isPaid: true },\n    ],\n    vision: [\n      { provider: 'opencode', model: 'big-pickle', isPaid: false },\n      { provider: 'openrouter', model: 'google/gemini-2.5-flash', isPaid: false },\n      { provider: 'openrouter', model: 'anthropic/claude-sonnet-4', isPaid: true },\n      { provider: 'openrouter', model: 'openai/gpt-4o', isPaid: true },\n    ],\n  };\n\n  const allModels = tierModels[tier] || tierModels.medium;\n  \n  // Filter to only models with available API keys\n  return allModels.filter(({ provider }) => {\n    const apiKey = getApiKey(provider);\n    return !!apiKey;\n  });\n}\n\n/**\n * Route to provider with fallback capability\n * Returns { success: true } if successful, { success: false, shouldContinue: true } to try next\n */\nasync function routeToProviderWithFallback(\n  req: IncomingMessage,\n  res: ServerResponse,\n  request: Record<string, unknown>,\n  provider: LLMProvider,\n  modelId: string,\n  apiKey: string\n): Promise<{ success: boolean }> {\n  try {\n    const baseUrl = PROVIDER_URLS[provider];\n    const providerRequest = buildProviderRequest(request, provider, modelId);\n    \n    console.log(`[OmniLLM] Attempting ${provider}/${modelId}`);\n    \n    const response = await fetch(`${baseUrl}/chat/completions`, {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": `Bearer ${apiKey}`,\n        ...(provider === \"openrouter\" ? { \"HTTP-Referer\": \"http://localhost:8403\", \"X-Title\": \"OmniLLM\" } : {}),\n      },\n      body: JSON.stringify(providerRequest),\n    });\n    \n    if (!response.ok) {\n      const errorText = await response.text();\n      const isRateLimit = response.status === 429;\n      const isServerError = response.status >= 500;\n      \n      console.error(`[OmniLLM] ${provider}/${modelId} failed (${response.status})${isRateLimit ? ' [RATE LIMITED]' : ''}: ${errorText.substring(0, 200)}`);\n      \n      // For rate limits and server errors, allow rotation to next provider\n      if (isRateLimit || isServerError) {\n        return { success: false };\n      }\n      \n      // For other errors (4xx client errors), still return error but don't rotate\n      // as it's likely a request format issue\n      if (!res.headersSent) {\n        res.writeHead(response.status, { \"Content-Type\": \"application/json\" });\n        res.end(JSON.stringify({ \n          error: `Provider error (${provider}): ${errorText}`,\n          provider,\n          model: modelId \n        }));\n      }\n      return { success: false };\n    }\n    \n    // Success - stream response back\n    const isStreaming = request.stream === true;\n    \n    if (isStreaming && response.body) {\n      res.writeHead(200, {\n        \"Content-Type\": \"text/event-stream\",\n        \"Cache-Control\": \"no-cache\",\n        \"Connection\": \"keep-alive\",\n      });\n      \n      const reader = response.body.getReader();\n      \n      try {\n        while (true) {\n          const { done, value } = await reader.read();\n          if (done) break;\n          res.write(value);\n        }\n      } finally {\n        res.end();\n      }\n    } else {\n      const data = await response.text();\n      res.writeHead(200, { \"Content-Type\": \"application/json\" });\n      res.end(data);\n    }\n    \n    return { success: true };\n    \n  } catch (error) {\n    console.error(`[OmniLLM] Exception routing to ${provider}/${modelId}:`, error);\n    return { success: false };\n  }\n}\n\n/**\n * Get first available provider with API key\n */\nfunction getFirstAvailableProvider(): { provider: LLMProvider; modelId: string } | null {\n  // Check providers in priority order - Free providers first\n  const priority: LLMProvider[] = [\n    'opencode',    // Free models\n    'chutes',      // Cheaper/discounted models\n    'nvidia',      // Free tier\n    'openrouter',  // Free tier models\n    'openai', \n    'anthropic', \n    'google', \n    'xai', \n    'deepseek', \n    'moonshot'\n  ];\n  \n  for (const provider of priority) {\n    const apiKey = getApiKey(provider);\n    if (apiKey) {\n      // Return appropriate default model for provider\n      let modelId = 'gpt-4o';\n      if (provider === 'opencode') modelId = 'big-pickle';\n      else if (provider === 'chutes') modelId = 'chutes/llama-3.1-70b';\n      else if (provider === 'nvidia') modelId = 'nvidia/llama-3.1-nemotron-70b-instruct';\n      else if (provider === 'openrouter') modelId = 'google/gemini-2.5-flash';\n      else if (provider === 'anthropic') modelId = 'claude-sonnet-4';\n      else if (provider === 'google') modelId = 'gemini-2.5-flash';\n      else if (provider === 'xai') modelId = 'grok-3';\n      else if (provider === 'deepseek') modelId = 'deepseek-v3';\n      else if (provider === 'moonshot') modelId = 'kimi-k2.5';\n      \n      return { provider, modelId };\n    }\n  }\n  \n  return null;\n}\n\n/**\n * Resolve model to provider\n */\nfunction resolveProvider(model: string): { provider: LLMProvider; modelId: string } | null {\n  // Handle omni-llm/ prefix\n  if (model.startsWith(\"omni-llm/\")) {\n    model = model.replace(\"omni-llm/\", \"\");\n  }\n  \n  // Handle auto mode - classify request and route intelligently\n  if (model === 'auto' || model === 'omni-llm/auto') {\n    // This will be populated when handleChatCompletions is called\n    // For now, return a placeholder that will be resolved later\n    return { provider: 'opencode', modelId: 'big-pickle' };\n  }\n  \n  // Resolve alias\n  const resolvedModelId = resolveModelAlias(model);\n  \n  // Get model info\n  const modelInfo = getModel(resolvedModelId);\n  if (modelInfo) {\n    return { provider: modelInfo.provider, modelId: modelInfo.id };\n  }\n  \n  // Try to infer from prefix\n  for (const provider of Object.keys(PROVIDER_URLS) as LLMProvider[]) {\n    if (resolvedModelId.startsWith(provider) || resolvedModelId.startsWith(`${provider}/`)) {\n      return { \n        provider, \n        modelId: resolvedModelId.replace(`${provider}/`, \"\") \n      };\n    }\n  }\n  \n  // Default to first available provider instead of openrouter\n  const available = getFirstAvailableProvider();\n  if (available) {\n    return available;\n  }\n  \n  // Last resort - openrouter\n  return { provider: \"openrouter\", modelId: resolvedModelId };\n}\n\n/**\n * Get API key for provider\n */\nfunction getApiKey(provider: LLMProvider): string | undefined {\n  const envVar = PROVIDER_ENV_VARS[provider];\n  return process.env[envVar];\n}\n\n/**\n * Handle chat completions request\n */\nasync function handleChatCompletions(\n  req: IncomingMessage,\n  res: ServerResponse,\n  body: string\n): Promise<void> {\n  try {\n    const request = JSON.parse(body);\n    const model = request.model || \"auto\";\n    \n    console.log(`[OmniLLM] Request for model: ${model}`);\n    \n    // Smart routing for auto mode with rotation\n    if (model === 'auto' || model === 'omni-llm/auto') {\n      const tier = classifyRequest(request);\n      const models = getAllModelsForTier(tier);\n      \n      if (models.length === 0) {\n        res.writeHead(400, { \"Content-Type\": \"application/json\" });\n        res.end(JSON.stringify({ error: \"No providers available\" }));\n        return;\n      }\n      \n      console.log(`[OmniLLM] Smart routing: tier=${tier}, trying ${models.length} models`);\n      \n      // Try each model in order with rotation on failure\n      for (let i = 0; i < models.length; i++) {\n        const { provider, model: modelId, isPaid } = models[i];\n        const apiKey = getApiKey(provider);\n        \n        if (!apiKey) {\n          console.log(`[OmniLLM] Skipping ${provider}/${modelId} - no API key`);\n          continue;\n        }\n        \n        console.log(`[OmniLLM] Attempt ${i + 1}/${models.length}: ${provider}/${modelId} (${isPaid ? 'paid' : 'free'})`);\n        \n        const result = await routeToProviderWithFallback(req, res, request, provider, modelId, apiKey);\n        \n        if (result.success) {\n          console.log(`[OmniLLM] Success with ${provider}/${modelId}`);\n          return; // Request completed successfully\n        }\n        \n        // If this was the last option, we've already sent the error response\n        if (i === models.length - 1) {\n          console.log(`[OmniLLM] All ${models.length} models failed`);\n          return;\n        }\n        \n        console.log(`[OmniLLM] ${provider}/${modelId} failed, rotating to next...`);\n        // Reset response for next attempt (create new response object logic)\n        // Actually, we can't reset the response, so we need to handle this differently\n        // For now, just continue to next - the error was already sent to client\n        // In a real implementation, we'd buffer the response\n      }\n      \n      return;\n    }\n    \n    // Non-auto mode - single provider\n    const resolved = resolveProvider(model);\n    if (!resolved) {\n      res.writeHead(400, { \"Content-Type\": \"application/json\" });\n      res.end(JSON.stringify({ error: \"Unable to resolve model\" }));\n      return;\n    }\n    \n    const { provider, modelId } = resolved;\n    const apiKey = getApiKey(provider);\n    \n    if (!apiKey) {\n      const openrouterKey = getApiKey(\"openrouter\");\n      if (openrouterKey && provider !== \"openrouter\") {\n        console.log(`[OmniLLM] No API key for ${provider}, falling back to OpenRouter`);\n        await routeToOpenRouter(req, res, request, modelId, openrouterKey);\n        return;\n      }\n      \n      res.writeHead(400, { \"Content-Type\": \"application/json\" });\n      res.end(JSON.stringify({ \n        error: `No API key configured for provider: ${provider}. Set ${PROVIDER_ENV_VARS[provider]} environment variable.` \n      }));\n      return;\n    }\n    \n    await routeToProvider(req, res, request, provider, modelId, apiKey);\n    \n  } catch (error) {\n    console.error(\"[OmniLLM] Error handling request:\", error);\n    res.writeHead(500, { \"Content-Type\": \"application/json\" });\n    res.end(JSON.stringify({ \n      error: error instanceof Error ? error.message : \"Internal server error\" \n    }));\n  }\n}\n\n/**\n * Route request to specific provider\n */\nasync function routeToProvider(\n  req: IncomingMessage,\n  res: ServerResponse,\n  request: Record<string, unknown>,\n  provider: LLMProvider,\n  modelId: string,\n  apiKey: string\n): Promise<void> {\n  const baseUrl = PROVIDER_URLS[provider];\n  \n  // Build provider-specific request\n  const providerRequest = buildProviderRequest(request, provider, modelId);\n  \n  console.log(`[OmniLLM] Routing to ${provider} with model ${modelId}`);\n  \n  // Make request to provider\n  const response = await fetch(`${baseUrl}/chat/completions`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": `Bearer ${apiKey}`,\n      ...(provider === \"openrouter\" ? { \"HTTP-Referer\": \"http://localhost:8403\", \"X-Title\": \"OmniLLM\" } : {}),\n    },\n    body: JSON.stringify(providerRequest),\n  });\n  \n  if (!response.ok) {\n    const error = await response.text();\n    console.error(`[OmniLLM] Provider error (${response.status}):`, error);\n    res.writeHead(response.status, { \"Content-Type\": \"application/json\" });\n    res.end(JSON.stringify({ error: `Provider error: ${error}` }));\n    return;\n  }\n  \n  // Stream response back\n  const isStreaming = request.stream === true;\n  \n  if (isStreaming && response.body) {\n    res.writeHead(200, {\n      \"Content-Type\": \"text/event-stream\",\n      \"Cache-Control\": \"no-cache\",\n      \"Connection\": \"keep-alive\",\n    });\n    \n    const reader = response.body.getReader();\n    \n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        res.write(value);\n      }\n    } finally {\n      res.end();\n    }\n  } else {\n    const data = await response.text();\n    res.writeHead(200, { \"Content-Type\": \"application/json\" });\n    res.end(data);\n  }\n}\n\n/**\n * Route request through OpenRouter\n */\nasync function routeToOpenRouter(\n  req: IncomingMessage,\n  res: ServerResponse,\n  request: Record<string, unknown>,\n  modelId: string,\n  apiKey: string\n): Promise<void> {\n  const openrouterModel = modelId.includes(\"/\") ? modelId : `${getProviderFromModel(modelId)}/${modelId}`;\n  \n  console.log(`[OmniLLM] Routing through OpenRouter: ${openrouterModel}`);\n  \n  const response = await fetch(\"https://openrouter.ai/api/v1/chat/completions\", {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": `Bearer ${apiKey}`,\n      \"HTTP-Referer\": \"http://localhost:8403\",\n      \"X-Title\": \"OmniLLM\",\n    },\n    body: JSON.stringify({\n      ...request,\n      model: openrouterModel,\n    }),\n  });\n  \n  if (!response.ok) {\n    const error = await response.text();\n    console.error(`[OmniLLM] OpenRouter error (${response.status}):`, error);\n    res.writeHead(response.status, { \"Content-Type\": \"application/json\" });\n    res.end(JSON.stringify({ error: `OpenRouter error: ${error}` }));\n    return;\n  }\n  \n  const isStreaming = request.stream === true;\n  \n  if (isStreaming && response.body) {\n    res.writeHead(200, {\n      \"Content-Type\": \"text/event-stream\",\n      \"Cache-Control\": \"no-cache\",\n      \"Connection\": \"keep-alive\",\n    });\n    \n    const reader = response.body.getReader();\n    \n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        res.write(value);\n      }\n    } finally {\n      res.end();\n    }\n  } else {\n    const data = await response.text();\n    res.writeHead(200, { \"Content-Type\": \"application/json\" });\n    res.end(data);\n  }\n}\n\n/**\n * Build provider-specific request\n */\nfunction buildProviderRequest(\n  request: Record<string, unknown>,\n  provider: LLMProvider,\n  modelId: string\n): Record<string, unknown> {\n  const baseRequest = {\n    model: modelId,\n    messages: request.messages,\n    temperature: request.temperature,\n    max_tokens: request.max_tokens || request.maxTokens,\n    top_p: request.top_p || request.topP,\n    stream: request.stream,\n    // Pass tools for tool calling support\n    ...(request.tools && { tools: request.tools }),\n    ...(request.tool_choice && { tool_choice: request.tool_choice }),\n  };\n  \n  // Provider-specific adjustments\n  switch (provider) {\n    case \"chutes\":\n      // Chutes uses full model ID like \"tngtech/R1T2-Chimera-Speed\" or \"z-ai/glm5\"\n      return {\n        model: modelId,\n        messages: request.messages,\n        max_tokens: request.max_tokens || request.maxTokens || 4096,\n        temperature: request.temperature,\n        top_p: request.top_p || request.topP,\n        stream: request.stream,\n        // Pass tools for tool calling support\n        ...(request.tools && { tools: request.tools }),\n        ...(request.tool_choice && { tool_choice: request.tool_choice }),\n      };\n    case \"anthropic\":\n      return {\n        model: modelId,\n        messages: request.messages,\n        max_tokens: request.max_tokens || request.maxTokens || 4096,\n        temperature: request.temperature,\n        top_p: request.top_p || request.topP,\n        stream: request.stream,\n        // Pass tools for tool calling support\n        ...(request.tools && { tools: request.tools }),\n        ...(request.tool_choice && { tool_choice: request.tool_choice }),\n      };\n    case \"google\":\n      return {\n        model: modelId,\n        contents: (request.messages as Array<{role: string; content: string}>).map(m => ({\n          role: m.role === \"assistant\" ? \"model\" : m.role,\n          parts: [{ text: m.content }],\n        })),\n        generationConfig: {\n          temperature: request.temperature,\n          maxOutputTokens: request.max_tokens || request.maxTokens,\n          topP: request.top_p || request.topP,\n        },\n      };\n    default:\n      return baseRequest;\n  }\n}\n\n/**\n * Get provider name from model ID\n */\nfunction getProviderFromModel(modelId: string): string {\n  const prefixes: Record<string, string> = {\n    \"gpt\": \"openai\",\n    \"claude\": \"anthropic\",\n    \"gemini\": \"google\",\n    \"grok\": \"xai\",\n    \"deepseek\": \"deepseek\",\n    \"kimi\": \"moonshot\",\n    \"big-pickle\": \"opencode\",\n    \"glm5\": \"nvidia\",\n  };\n  \n  for (const [prefix, provider] of Object.entries(prefixes)) {\n    if (modelId.toLowerCase().includes(prefix)) {\n      return provider;\n    }\n  }\n  \n  return \"openrouter\";\n}\n\n/**\n * Read request body\n */\nasync function readBody(req: IncomingMessage): Promise<string> {\n  return new Promise((resolve, reject) => {\n    let body = \"\";\n    req.on(\"data\", (chunk) => {\n      body += chunk.toString();\n    });\n    req.on(\"end\", () => resolve(body));\n    req.on(\"error\", reject);\n  });\n}\n\n/**\n * Start the proxy server\n */\nexport async function startProxy(options: ProxyOptions = {}): Promise<ProxyHandle> {\n  const port = options.port || getProxyPort();\n  \n  // Check if already running\n  const existing = await checkExistingProxy(port);\n  if (existing) {\n    console.log(`[OmniLLM] Proxy already running on port ${port}`);\n    return {\n      port,\n      baseUrl: `http://127.0.0.1:${port}`,\n      close: async () => {},\n    };\n  }\n  \n  return new Promise((resolve, reject) => {\n    const server = createServer(async (req, res) => {\n      // CORS headers\n      res.setHeader(\"Access-Control-Allow-Origin\", \"*\");\n      res.setHeader(\"Access-Control-Allow-Methods\", \"GET, POST, OPTIONS\");\n      res.setHeader(\"Access-Control-Allow-Headers\", \"Content-Type, Authorization\");\n      \n      if (req.method === \"OPTIONS\") {\n        res.writeHead(200);\n        res.end();\n        return;\n      }\n      \n      // Health check\n      if (req.url === \"/health\" && req.method === \"GET\") {\n        res.writeHead(200, { \"Content-Type\": \"application/json\" });\n        res.end(JSON.stringify({ status: \"ok\", provider: \"omni-llm\" }));\n        return;\n      }\n      \n      // Models list\n      if (req.url === \"/v1/models\" && req.method === \"GET\") {\n        res.writeHead(200, { \"Content-Type\": \"application/json\" });\n        res.end(JSON.stringify({\n          object: \"list\",\n          data: Object.keys(getModel).map((id) => ({\n            id,\n            object: \"model\",\n            created: Date.now(),\n            owned_by: \"omni-llm\",\n          })),\n        }));\n        return;\n      }\n      \n      // Chat completions\n      if (req.url === \"/v1/chat/completions\" && req.method === \"POST\") {\n        try {\n          const body = await readBody(req);\n          await handleChatCompletions(req, res, body);\n        } catch (error) {\n          console.error(\"[OmniLLM] Error:\", error);\n          res.writeHead(500, { \"Content-Type\": \"application/json\" });\n          res.end(JSON.stringify({ error: \"Internal server error\" }));\n        }\n        return;\n      }\n      \n      // 404\n      res.writeHead(404, { \"Content-Type\": \"application/json\" });\n      res.end(JSON.stringify({ error: \"Not found\" }));\n    });\n    \n    server.listen(port, \"127.0.0.1\", () => {\n      const address = server.address() as AddressInfo;\n      console.log(`[OmniLLM] Proxy listening on port ${address.port}`);\n      \n      if (options.onReady) {\n        options.onReady(address.port);\n      }\n      \n      resolve({\n        port: address.port,\n        baseUrl: `http://127.0.0.1:${address.port}`,\n        close: () => {\n          return new Promise((res) => {\n            server.close(() => res());\n          });\n        },\n      });\n    });\n    \n    server.on(\"error\", (error) => {\n      console.error(\"[OmniLLM] Server error:\", error);\n      if (options.onError) {\n        options.onError(error);\n      }\n      reject(error);\n    });\n  });\n}\n","/**\n * OmniLLM - OpenClaw Provider Plugin\n * \n * Multi-provider LLM router that registers as an OpenClaw provider.\n * Supports OpenRouter, OpenAI, Anthropic, Google, xAI, DeepSeek, Moonshot, OpenCode, and more.\n * \n * Usage:\n *   openclaw plugins install ./omni-provider\n *   export OPENROUTER_API_KEY=sk-or-...\n *   openclaw models set omni-llm/auto\n */\n\nimport type {\n  OpenClawPluginDefinition,\n  OpenClawPluginApi,\n  PluginCommandContext,\n  OpenClawPluginCommandDefinition,\n} from \"./types/openclaw.js\";\nimport { omniLLMProvider, getProxyPort } from \"./provider.js\";\nimport { VERSION } from \"./version.js\";\nimport { OPENCLAW_MODELS, buildProviderModels } from \"./models.js\";\nimport { loadProviderConfig, loadApiKeysFromEnv } from \"./config/index.js\";\nimport { getStats, formatStatsAscii } from \"./stats.js\";\nimport { startProxy, type ProxyHandle } from \"./proxy.js\";\nimport { readFileSync, writeFileSync, existsSync, mkdirSync } from \"node:fs\";\nimport { homedir } from \"node:os\";\nimport { join } from \"node:path\";\n\n// Track active proxy handle for cleanup\nlet activeProxyHandle: ProxyHandle | null = null;\n\n/**\n * Start the proxy server in background\n */\nasync function startProxyServer(api: OpenClawPluginApi): Promise<void> {\n  try {\n    const proxy = await startProxy({\n      port: getProxyPort(),\n      onReady: (port) => {\n        api.logger.info(`OmniLLM proxy listening on port ${port}`);\n      },\n      onError: (error) => {\n        api.logger.error(`OmniLLM proxy error: ${error.message}`);\n      },\n    });\n    \n    activeProxyHandle = proxy;\n    \n    // Health check\n    const healthy = await waitForProxyHealth(proxy.port, 5000);\n    if (!healthy) {\n      api.logger.warn(\"OmniLLM proxy health check timed out\");\n    }\n  } catch (err) {\n    api.logger.error(`Failed to start OmniLLM proxy: ${err instanceof Error ? err.message : String(err)}`);\n    throw err;\n  }\n}\n\n/**\n * Wait for proxy to be healthy\n */\nasync function waitForProxyHealth(port: number, timeoutMs = 3000): Promise<boolean> {\n  const start = Date.now();\n  while (Date.now() - start < timeoutMs) {\n    try {\n      const res = await fetch(`http://127.0.0.1:${port}/health`);\n      if (res.ok) return true;\n    } catch { /* not ready */ }\n    await new Promise((r) => setTimeout(r, 100));\n  }\n  return false;\n}\n\n/**\n * Check if running in completion mode (direct model usage)\n */\nfunction isCompletionMode(): boolean {\n  return process.argv.some((arg, i) => arg === \"completion\" && i >= 1 && i <= 3);\n}\n\n/**\n * Inject OmniLLM models into OpenClaw config file\n * This ensures the models are available in the UI\n */\nfunction injectModelsConfig(logger: { info: (msg: string) => void }): void {\n  const configDir = join(homedir(), \".openclaw\");\n  const configPath = join(configDir, \"openclaw.json\");\n\n  let config: Record<string, unknown> = {};\n  let needsWrite = false;\n\n  if (!existsSync(configDir)) {\n    try {\n      mkdirSync(configDir, { recursive: true });\n    } catch {\n      return;\n    }\n  }\n\n  if (existsSync(configPath)) {\n    try {\n      const content = readFileSync(configPath, \"utf-8\").trim();\n      if (content) config = JSON.parse(content);\n      else needsWrite = true;\n    } catch {\n      config = {};\n      needsWrite = true;\n    }\n  } else {\n    needsWrite = true;\n  }\n\n  if (!config.models) {\n    config.models = {};\n    needsWrite = true;\n  }\n  const models = config.models as Record<string, unknown>;\n  if (!models.providers) {\n    models.providers = {};\n    needsWrite = true;\n  }\n\n  const proxyPort = getProxyPort();\n  const expectedBaseUrl = `http://127.0.0.1:${proxyPort}/v1`;\n  const providers = models.providers as Record<string, unknown>;\n\n  if (!providers[\"omni-llm\"]) {\n    providers[\"omni-llm\"] = {\n      baseUrl: expectedBaseUrl,\n      api: \"openai-completions\",\n      apiKey: \"local-proxy\",\n      models: OPENCLAW_MODELS,\n    };\n    needsWrite = true;\n  } else {\n    const omni = providers[\"omni-llm\"] as Record<string, unknown>;\n    let fixed = false;\n    if (!omni.baseUrl || omni.baseUrl !== expectedBaseUrl) {\n      omni.baseUrl = expectedBaseUrl;\n      fixed = true;\n    }\n    if (!omni.api) {\n      omni.api = \"openai-completions\";\n      fixed = true;\n    }\n    if (!omni.apiKey) {\n      omni.apiKey = \"local-proxy\";\n      fixed = true;\n    }\n    const currentModels = omni.models as unknown[];\n    if (!currentModels || !Array.isArray(currentModels) || currentModels.length !== OPENCLAW_MODELS.length) {\n      omni.models = OPENCLAW_MODELS;\n      fixed = true;\n    }\n    if (fixed) needsWrite = true;\n  }\n\n  // Set default model on first install\n  if (!config.agents) {\n    config.agents = {};\n    needsWrite = true;\n  }\n  const agents = config.agents as Record<string, unknown>;\n  if (!agents.defaults) {\n    agents.defaults = {};\n    needsWrite = true;\n  }\n  const defaults = agents.defaults as Record<string, unknown>;\n  if (!defaults.model) {\n    defaults.model = {};\n    needsWrite = true;\n  }\n  const model = defaults.model as Record<string, unknown>;\n  if (!model.primary) {\n    model.primary = \"omni-llm/auto\";\n    needsWrite = true;\n  }\n\n  // Register model aliases\n  const KEY_ALIASES = [\n    { id: \"auto\", alias: \"auto\" },\n    { id: \"sonnet\", alias: \"sonnet\" },\n    { id: \"opus\", alias: \"opus\" },\n    { id: \"haiku\", alias: \"haiku\" },\n    { id: \"gpt\", alias: \"gpt\" },\n    { id: \"flash\", alias: \"flash\" },\n    { id: \"deepseek\", alias: \"deepseek\" },\n    { id: \"grok\", alias: \"grok\" },\n    { id: \"kimi\", alias: \"kimi\" },\n    { id: \"reasoner\", alias: \"reasoner\" },\n  ];\n\n  if (!defaults.models) {\n    defaults.models = {};\n    needsWrite = true;\n  }\n  const allowlist = defaults.models as Record<string, unknown>;\n  for (const m of KEY_ALIASES) {\n    const fullId = `omni-llm/${m.id}`;\n    if (!allowlist[fullId]) {\n      allowlist[fullId] = { alias: m.alias };\n      needsWrite = true;\n    }\n  }\n\n  if (needsWrite) {\n    try {\n      writeFileSync(configPath, JSON.stringify(config, null, 2));\n      logger.info(`Updated OpenClaw config with OmniLLM models`);\n    } catch {\n      // Ignore write errors\n    }\n  }\n}\n\n/**\n * Create the stats command\n */\nasync function createStatsCommand(): Promise<OpenClawPluginCommandDefinition> {\n  return {\n    name: \"stats\",\n    description: \"Show OmniLLM usage statistics and cost savings\",\n    acceptsArgs: true,\n    requireAuth: false,\n    handler: async (ctx: PluginCommandContext) => {\n      const days = parseInt(ctx.args?.trim() || \"7\", 10) || 7;\n      try {\n        const stats = await getStats(Math.min(days, 30));\n        return { text: [\"```\", formatStatsAscii(stats), \"```\"].join(\"\\n\") };\n      } catch (err) {\n        return {\n          text: `Failed to load stats: ${err instanceof Error ? err.message : String(err)}`,\n          isError: true,\n        };\n      }\n    },\n  };\n}\n\n/**\n * Create the providers command to show configured API keys\n */\nasync function createProvidersCommand(api: OpenClawPluginApi): Promise<OpenClawPluginCommandDefinition> {\n  return {\n    name: \"providers\",\n    description: \"Show configured OmniLLM provider status\",\n    acceptsArgs: false,\n    requireAuth: false,\n    handler: async () => {\n      const apiKeys = loadApiKeysFromEnv();\n      const providers = Object.entries(apiKeys)\n        .filter(([_, cfg]) => cfg && typeof cfg === \"object\" && \"apiKey\" in cfg && cfg.apiKey)\n        .map(([name, cfg]) => {\n          const config = cfg as { apiKey?: string };\n          const key = config.apiKey || \"\";\n          const masked = key.length > 8 ? `${key.slice(0, 4)}...${key.slice(-4)}` : \"****\";\n          return `â€¢ **${name}**: \\`${masked}\\` âœ…`;\n        });\n\n      if (providers.length === 0) {\n        return {\n          text: [\n            \"ðŸ”‘ **OmniLLM API Keys**\",\n            \"\",\n            \"No API keys configured!\",\n            \"\",\n            \"**Quickest setup (one key â†’ all models):**\",\n            \"â€¢ `OPENROUTER_API_KEY=sk-or-...`\",\n            \"\",\n            \"**Or configure individual providers:**\",\n            \"â€¢ `OPENAI_API_KEY=sk-...`\",\n            \"â€¢ `ANTHROPIC_API_KEY=sk-ant-...`\",\n            \"â€¢ `GOOGLE_API_KEY=AIza...`\",\n            \"â€¢ `XAI_API_KEY=xai-...`\",\n            \"â€¢ `DEEPSEEK_API_KEY=sk-...`\",\n            \"\",\n            \"**Or edit:** `~/.openclaw/omni-llm/config.json`\",\n          ].join(\"\\n\"),\n        };\n      }\n\n      return {\n        text: [\n          \"ðŸ”‘ **OmniLLM API Keys**\",\n          \"\",\n          ...providers,\n          \"\",\n          `**${providers.length} providers configured**`,\n        ].join(\"\\n\"),\n      };\n    },\n  };\n}\n\n// Main plugin definition\nconst plugin: OpenClawPluginDefinition = {\n  id: \"omni-llm\",\n  name: \"OmniLLM\",\n  description: \"Multi-provider LLM router â€” your keys, smart routing, maximum flexibility\",\n  version: VERSION,\n\n  register(api: OpenClawPluginApi) {\n    const isDisabled = process.env.OMNI_LLM_DISABLED === \"true\" || process.env.OMNI_LLM_DISABLED === \"1\";\n    if (isDisabled) {\n      api.logger.info(\"OmniLLM disabled (OMNI_LLM_DISABLED=true)\");\n      return;\n    }\n\n    // In completion mode, just register the provider\n    if (isCompletionMode()) {\n      api.registerProvider(omniLLMProvider);\n      return;\n    }\n\n    // Load configuration\n    const { providers } = loadProviderConfig(api.pluginConfig || {});\n    const apiKeys = loadApiKeysFromEnv();\n\n    const enabledProviders = Object.entries(providers)\n      .filter(([_, cfg]) => cfg.enabled)\n      .map(([provider]) => provider);\n\n    if (enabledProviders.length === 0) {\n      api.logger.warn(\"OmniLLM: No providers enabled! Configure at least one provider.\");\n      return;\n    }\n\n    api.logger.info(`OmniLLM: Configured providers: ${enabledProviders.join(\", \")}`);\n\n    // Register the provider with OpenClaw\n    api.registerProvider(omniLLMProvider);\n\n    // Inject models into OpenClaw config\n    injectModelsConfig(api.logger);\n\n    // Configure runtime models\n    const runtimePort = getProxyPort();\n    if (!api.config.models) api.config.models = { providers: {} };\n    if (!api.config.models.providers) api.config.models.providers = {};\n    api.config.models.providers[\"omni-llm\"] = {\n      baseUrl: `http://127.0.0.1:${runtimePort}/v1`,\n      api: \"openai-completions\",\n      apiKey: \"local-proxy\",\n      models: OPENCLAW_MODELS,\n    };\n\n    // Set default model if not configured\n    if (!api.config.agents) api.config.agents = {};\n    const agents = api.config.agents as Record<string, unknown>;\n    if (!agents.defaults) agents.defaults = {};\n    const defaults = agents.defaults as Record<string, unknown>;\n    if (!defaults.model) defaults.model = {};\n    const model = defaults.model as Record<string, unknown>;\n    if (!model.primary) model.primary = \"omni-llm/auto\";\n\n    api.logger.info(`OmniLLM registered (${enabledProviders.length} providers)`);\n\n    // Register commands\n    createStatsCommand()\n      .then((cmd) => api.registerCommand(cmd))\n      .catch(() => {});\n    createProvidersCommand(api)\n      .then((cmd) => api.registerCommand(cmd))\n      .catch(() => {});\n\n    // Register service for cleanup\n    api.registerService({\n      id: \"omni-llm-service\",\n      start: async () => {\n        await startProxyServer(api);\n      },\n      stop: async () => {\n        if (activeProxyHandle) {\n          try {\n            await activeProxyHandle.close();\n          } catch {\n            // Ignore cleanup errors\n          }\n          activeProxyHandle = null;\n        }\n      },\n    });\n\n    // Start proxy immediately\n    startProxyServer(api).catch((err) => {\n      api.logger.error(`Failed to start proxy: ${err instanceof Error ? err.message : String(err)}`);\n    });\n  },\n};\n\nexport default plugin;\n\n// Re-exports for library usage\nexport { omniLLMProvider, getProxyPort } from \"./provider.js\";\nexport { startProxy } from \"./proxy.js\";\nexport type { ProxyHandle } from \"./proxy.js\";\nexport {\n  OPENCLAW_MODELS,\n  MODEL_ALIASES,\n  buildProviderModels,\n  resolveModelAlias,\n  getModel,\n  getModelsByProvider,\n  getFreeModels,\n  isAgenticModel,\n  getAgenticModels,\n  getModelContextWindow,\n} from \"./models.js\";\nexport { VERSION } from \"./version.js\";\nexport { getStats, formatStatsAscii } from \"./stats.js\";\nexport type { DailyStats } from \"./stats.js\";\nexport type { AggregatedStats } from \"./types.js\";\n"],"mappings":";AAUO,IAAM,SAAoC;AAAA;AAAA,EAE/C,UAAU;AAAA,IACR,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,QAAQ,KAAK;AAAA,IACrE,SAAS,EAAE,OAAO,GAAK,QAAQ,GAAK;AAAA,EACtC;AAAA,EACA,eAAe;AAAA,IACb,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,QAAQ,KAAK;AAAA,IACrE,SAAS,EAAE,OAAO,MAAM,QAAQ,IAAI;AAAA,EACtC;AAAA,EACA,SAAS;AAAA,IACP,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,QAAQ,MAAM,WAAW,KAAK;AAAA,IACtF,SAAS,EAAE,OAAO,IAAM,QAAQ,GAAK;AAAA,EACvC;AAAA,EACA,MAAM;AAAA,IACJ,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,OAAO,iBAAiB,MAAM,WAAW,KAAK;AAAA,IACzE,SAAS,EAAE,OAAO,IAAM,QAAQ,GAAK;AAAA,EACvC;AAAA,EACA,WAAW;AAAA,IACT,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,OAAO,iBAAiB,MAAM,WAAW,KAAK;AAAA,IACzE,SAAS,EAAE,OAAO,KAAK,QAAQ,IAAI;AAAA,EACrC;AAAA;AAAA,EAGA,mBAAmB;AAAA,IACjB,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,QAAQ,KAAK;AAAA,IACrE,SAAS,EAAE,OAAO,GAAK,QAAQ,GAAK;AAAA,EACtC;AAAA,EACA,iBAAiB;AAAA,IACf,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,QAAQ,KAAK;AAAA,IACrE,SAAS,EAAE,OAAO,IAAM,QAAQ,GAAK;AAAA,EACvC;AAAA,EACA,kBAAkB;AAAA,IAChB,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,QAAQ,KAAK;AAAA,IACrE,SAAS,EAAE,OAAO,KAAK,QAAQ,EAAI;AAAA,EACrC;AAAA;AAAA,EAGA,kBAAkB;AAAA,IAChB,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,QAAQ,MAAM,WAAW,KAAK;AAAA,IACtF,SAAS,EAAE,OAAO,MAAM,QAAQ,EAAI;AAAA,EACtC;AAAA,EACA,oBAAoB;AAAA,IAClB,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,QAAQ,KAAK;AAAA,IACrE,SAAS,EAAE,OAAO,OAAO,QAAQ,IAAI;AAAA,EACvC;AAAA;AAAA,EAGA,UAAU;AAAA,IACR,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,WAAW,KAAK;AAAA,IACxE,SAAS,EAAE,OAAO,GAAK,QAAQ,GAAK;AAAA,EACtC;AAAA,EACA,eAAe;AAAA,IACb,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,KAAK;AAAA,IACvD,SAAS,EAAE,OAAO,GAAK,QAAQ,GAAK;AAAA,EACtC;AAAA,EACA,UAAU;AAAA,IACR,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,KAAK;AAAA,IACvD,SAAS,EAAE,OAAO,GAAK,QAAQ,GAAK;AAAA,EACtC;AAAA;AAAA,EAGA,eAAe;AAAA,IACb,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,KAAK;AAAA,IACvD,SAAS,EAAE,OAAO,MAAM,QAAQ,IAAI;AAAA,EACtC;AAAA,EACA,iBAAiB;AAAA,IACf,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,KAAK;AAAA,IAChC,SAAS,EAAE,OAAO,MAAM,QAAQ,KAAK;AAAA,EACvC;AAAA;AAAA,EAGA,aAAa;AAAA,IACX,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,QAAQ,KAAK;AAAA,IACrE,SAAS,EAAE,OAAO,KAAK,QAAQ,EAAI;AAAA,EACrC;AAAA,EACA,kBAAkB;AAAA,IAChB,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,QAAQ,KAAK;AAAA,IACrE,SAAS,EAAE,OAAO,KAAK,QAAQ,IAAI;AAAA,EACrC;AAAA;AAAA,EAGA,cAAc;AAAA,IACZ,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,MAAM,WAAW,KAAK;AAAA,IACxE,SAAS,EAAE,OAAO,GAAK,QAAQ,EAAI;AAAA,EACrC;AAAA,EACA,cAAc;AAAA,IACZ,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,KAAK;AAAA,IACvD,SAAS,EAAE,OAAO,GAAK,QAAQ,EAAI;AAAA,EACrC;AAAA,EACA,kBAAkB;AAAA,IAChB,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,KAAK;AAAA,IACvD,SAAS,EAAE,OAAO,GAAK,QAAQ,EAAI;AAAA,EACrC;AAAA,EACA,qBAAqB;AAAA,IACnB,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,KAAK;AAAA,IACvD,SAAS,EAAE,OAAO,GAAK,QAAQ,EAAI;AAAA,EACrC;AAAA;AAAA,EAGA,mBAAmB;AAAA,IACjB,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,KAAK;AAAA,EACzD;AAAA;AAAA,EAGA,aAAa;AAAA,IACX,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,KAAK;AAAA,IACvD,SAAS,EAAE,OAAO,GAAK,QAAQ,EAAI;AAAA,EACrC;AAAA,EACA,0CAA0C;AAAA,IACxC,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,KAAK;AAAA,IACvD,SAAS,EAAE,OAAO,GAAK,QAAQ,EAAI;AAAA,EACrC;AAAA,EACA,wBAAwB;AAAA,IACtB,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,UAAU;AAAA,IACV,eAAe;AAAA,IACf,iBAAiB;AAAA,IACjB,cAAc,EAAE,WAAW,MAAM,iBAAiB,KAAK;AAAA,IACvD,SAAS,EAAE,OAAO,GAAK,QAAQ,EAAI;AAAA,EACrC;AACF;AAGO,IAAM,gBAA8B;AAAA,EACzC,EAAE,OAAO,QAAQ,OAAO,gBAAgB;AAAA,EACxC,EAAE,OAAO,UAAU,OAAO,kBAAkB;AAAA,EAC5C,EAAE,OAAO,QAAQ,OAAO,gBAAgB;AAAA,EACxC,EAAE,OAAO,SAAS,OAAO,iBAAiB;AAAA,EAC1C,EAAE,OAAO,OAAO,OAAO,SAAS;AAAA,EAChC,EAAE,OAAO,YAAY,OAAO,cAAc;AAAA,EAC1C,EAAE,OAAO,SAAS,OAAO,mBAAmB;AAAA,EAC5C,EAAE,OAAO,OAAO,OAAO,iBAAiB;AAAA,EACxC,EAAE,OAAO,YAAY,OAAO,cAAc;AAAA,EAC1C,EAAE,OAAO,QAAQ,OAAO,SAAS;AAAA,EACjC,EAAE,OAAO,kBAAkB,OAAO,cAAc;AAAA,EAChD,EAAE,OAAO,QAAQ,OAAO,YAAY;AAAA,EACpC,EAAE,OAAO,cAAc,OAAO,cAAc,UAAU,WAAW;AAAA,EACjE,EAAE,OAAO,YAAY,OAAO,cAAc,UAAU,WAAW;AAAA,EAC/D,EAAE,OAAO,YAAY,OAAO,UAAU;AAAA,EACtC,EAAE,OAAO,aAAa,OAAO,KAAK;AAAA,EAClC,EAAE,OAAO,QAAQ,OAAO,YAAY;AAAA,EACpC,EAAE,OAAO,YAAY,OAAO,yCAAyC;AACvE;AAGA,SAAS,gBAAgB,GAAqC;AAC5D,SAAO;AAAA,IACL,IAAI,EAAE;AAAA,IACN,MAAM,EAAE;AAAA,IACR,KAAK;AAAA,IACL,WAAW,EAAE,cAAc,aAAa;AAAA,IACxC,OAAO,EAAE,cAAc,SAAS,CAAC,QAAQ,OAAO,IAAI,CAAC,MAAM;AAAA,IAC3D,MAAM;AAAA,MACJ,OAAO,EAAE,SAAS,SAAS;AAAA,MAC3B,QAAQ,EAAE,SAAS,UAAU;AAAA,MAC7B,WAAW;AAAA,MACX,YAAY;AAAA,IACd;AAAA,IACA,eAAe,EAAE;AAAA,IACjB,WAAW,EAAE;AAAA,EACf;AACF;AAGA,IAAM,eAAwC,cAC3C,IAAI,CAAC,UAAU;AACd,QAAM,SAAS,OAAO,MAAM,MAAM,QAAQ,aAAa,EAAE,CAAC;AAC1D,MAAI,CAAC,OAAQ,QAAO;AACpB,SAAO,gBAAgB,EAAE,GAAG,QAAQ,IAAI,MAAM,OAAO,MAAM,GAAG,MAAM,KAAK,WAAM,OAAO,IAAI,GAAG,CAAC;AAChG,CAAC,EACA,OAAO,CAAC,MAAkC,MAAM,IAAI;AAGhD,IAAM,kBAA2C;AAAA;AAAA,EAEtD;AAAA,IACE,IAAI;AAAA,IACJ,MAAM;AAAA,IACN,KAAK;AAAA,IACL,WAAW;AAAA,IACX,OAAO,CAAC,MAAM;AAAA,IACd,MAAM,EAAE,OAAO,GAAG,QAAQ,GAAG,WAAW,GAAG,YAAY,EAAE;AAAA,IACzD,eAAe;AAAA,IACf,WAAW;AAAA,EACb;AAAA;AAAA,EAEA,GAAG,OAAO,OAAO,MAAM,EAAE,IAAI,eAAe;AAAA;AAAA,EAE5C,GAAG;AACL;AA2BO,SAAS,oBAAoB,SAAsC;AACxE,SAAO;AAAA,IACL,SAAS,GAAG,OAAO;AAAA,IACnB,KAAK;AAAA,IACL,QAAQ;AAAA,IACR,QAAQ;AAAA,EACV;AACF;AAKO,SAAS,SAAS,SAAwC;AAC/D,SAAO,OAAO,OAAO;AACvB;AAKO,SAAS,kBAAkB,OAAuB;AACvD,QAAM,QAAQ,cAAc,KAAK,CAAC,MAAM,EAAE,UAAU,KAAK;AACzD,SAAO,OAAO,SAAS;AACzB;AAKO,SAAS,oBAAoB,UAAoC;AACtE,SAAO,OAAO,OAAO,MAAM,EAAE,OAAO,CAAC,MAAM,EAAE,aAAa,QAAQ;AACpE;AAKO,SAAS,gBAA6B;AAC3C,SAAO,OAAO,OAAO,MAAM,EAAE,OAAO,CAAC,MAAM,EAAE,SAAS,UAAU,CAAC;AACnE;AAKO,SAAS,eAAe,SAA0B;AACvD,QAAM,QAAQ,OAAO,OAAO;AAC5B,SAAO,OAAO,cAAc,mBAAmB;AACjD;AAKO,SAAS,mBAA6B;AAC3C,SAAO,OAAO,OAAO,MAAM,EACxB,OAAO,CAAC,MAAM,EAAE,cAAc,eAAe,EAC7C,IAAI,CAAC,MAAM,EAAE,EAAE;AACpB;AAKO,SAAS,sBAAsB,SAAqC;AACzE,SAAO,OAAO,OAAO,GAAG;AAC1B;;;AC7YA,IAAM,eAAe;AAEd,SAAS,eAAuB;AACrC,QAAM,UAAU,QAAQ,IAAI;AAC5B,MAAI,SAAS;AACX,UAAM,SAAS,SAAS,SAAS,EAAE;AACnC,QAAI,CAAC,MAAM,MAAM,KAAK,SAAS,KAAK,SAAS,MAAO,QAAO;AAAA,EAC7D;AACA,SAAO;AACT;AAEO,IAAM,kBAAkC;AAAA,EAC7C,IAAI;AAAA,EACJ,OAAO;AAAA,EACP,UAAU;AAAA,EACV,SAAS,CAAC,MAAM;AAAA,EAChB,SAAS;AAAA,IACP;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAAA,EAEA,IAAI,SAAS;AACX,UAAM,OAAO,aAAa;AAC1B,WAAO,oBAAoB,oBAAoB,IAAI,EAAE;AAAA,EACvD;AAAA,EAEA,MAAM,CAAC;AACT;;;AC5CO,IAAM,UAAU;;;AC6FhB,SAAS,mBAAmB,cAA2G;AAC5I,QAAM,gBAAiC;AAAA,IACrC,YAAY,EAAE,SAAS,MAAM;AAAA,IAC7B,QAAQ,EAAE,SAAS,MAAM;AAAA,IACzB,WAAW,EAAE,SAAS,MAAM;AAAA,IAC5B,QAAQ,EAAE,SAAS,MAAM;AAAA,IACzB,KAAK,EAAE,SAAS,MAAM;AAAA,IACtB,UAAU,EAAE,SAAS,MAAM;AAAA,IAC3B,UAAU,EAAE,SAAS,MAAM;AAAA,IAC3B,UAAU,EAAE,SAAS,MAAM;AAAA,IAC3B,OAAO,EAAE,SAAS,MAAM;AAAA,IACxB,UAAU,EAAE,SAAS,MAAM;AAAA,IAC3B,UAAU,EAAE,SAAS,MAAM;AAAA,IAC3B,WAAW,EAAE,SAAS,MAAM;AAAA,IAC5B,SAAS,EAAE,SAAS,MAAM;AAAA,IAC1B,QAAQ,EAAE,SAAS,MAAM;AAAA,IACzB,YAAY,EAAE,SAAS,MAAM;AAAA,IAC7B,QAAQ,EAAE,SAAS,MAAM;AAAA,EAC3B;AAEA,MAAI,SAA0B,EAAE,GAAG,cAAc;AAGjD,MAAI,cAAc,WAAW;AAC3B,UAAM,YAAY,aAAa;AAC/B,eAAW,CAAC,UAAU,GAAG,KAAK,OAAO,QAAQ,SAAS,GAAG;AACvD,UAAI,gBAAgB,QAAQ,GAAG;AAC7B,eAAO,QAAQ,IAAI,EAAE,GAAG,cAAc,QAAQ,GAAG,GAAG,IAAI;AAAA,MAC1D;AAAA,IACF;AAAA,EACF;AAGA,WAAS,kBAAkB,MAAM;AAEjC,SAAO,EAAE,WAAW,QAAQ,SAAS,cAAc,QAA+C;AACpG;AAKA,SAAS,kBAAkB,QAA0C;AAEnE,MAAI,QAAQ,IAAI,oBAAoB;AAClC,WAAO,aAAa,EAAE,GAAG,OAAO,YAAY,SAAS,MAAM,QAAQ,QAAQ,IAAI,mBAAmB;AAAA,EACpG;AAGA,MAAI,QAAQ,IAAI,gBAAgB;AAC9B,WAAO,SAAS,EAAE,GAAG,OAAO,QAAQ,SAAS,MAAM,QAAQ,QAAQ,IAAI,eAAe;AAAA,EACxF;AAGA,MAAI,QAAQ,IAAI,mBAAmB;AACjC,WAAO,YAAY,EAAE,GAAG,OAAO,WAAW,SAAS,MAAM,QAAQ,QAAQ,IAAI,kBAAkB;AAAA,EACjG;AAGA,MAAI,QAAQ,IAAI,gBAAgB;AAC9B,WAAO,SAAS,EAAE,GAAG,OAAO,QAAQ,SAAS,MAAM,QAAQ,QAAQ,IAAI,eAAe;AAAA,EACxF;AAGA,MAAI,QAAQ,IAAI,aAAa;AAC3B,WAAO,MAAM,EAAE,GAAG,OAAO,KAAK,SAAS,MAAM,QAAQ,QAAQ,IAAI,YAAY;AAAA,EAC/E;AAGA,MAAI,QAAQ,IAAI,kBAAkB;AAChC,WAAO,WAAW,EAAE,GAAG,OAAO,UAAU,SAAS,MAAM,QAAQ,QAAQ,IAAI,iBAAiB;AAAA,EAC9F;AAGA,MAAI,QAAQ,IAAI,kBAAkB;AAChC,WAAO,WAAW,EAAE,GAAG,OAAO,UAAU,SAAS,MAAM,QAAQ,QAAQ,IAAI,iBAAiB;AAAA,EAC9F;AAGA,MAAI,QAAQ,IAAI,kBAAkB;AAChC,WAAO,WAAW,EAAE,GAAG,OAAO,UAAU,SAAS,MAAM,QAAQ,QAAQ,IAAI,iBAAiB;AAAA,EAC9F;AAGA,MAAI,QAAQ,IAAI,wBAAwB,QAAQ,IAAI,uBAAuB;AACzE,WAAO,QAAQ;AAAA,MACb,GAAG,OAAO;AAAA,MACV,SAAS;AAAA,MACT,QAAQ,QAAQ,IAAI;AAAA,MACpB,SAAS,QAAQ,IAAI;AAAA,IACvB;AAAA,EACF;AAGA,MAAI,QAAQ,IAAI,kBAAkB;AAChC,WAAO,WAAW,EAAE,GAAG,OAAO,UAAU,SAAS,MAAM,QAAQ,QAAQ,IAAI,iBAAiB;AAAA,EAC9F;AAGA,MAAI,QAAQ,IAAI,kBAAkB;AAChC,WAAO,WAAW,EAAE,GAAG,OAAO,UAAU,SAAS,MAAM,QAAQ,QAAQ,IAAI,iBAAiB;AAAA,EAC9F;AAGA,MAAI,QAAQ,IAAI,mBAAmB;AACjC,WAAO,YAAY,EAAE,GAAG,OAAO,WAAW,SAAS,MAAM,QAAQ,QAAQ,IAAI,kBAAkB;AAAA,EACjG;AAGA,MAAI,QAAQ,IAAI,iBAAiB;AAC/B,WAAO,UAAU,EAAE,GAAG,OAAO,SAAS,SAAS,MAAM,QAAQ,QAAQ,IAAI,gBAAgB;AAAA,EAC3F;AAGA,MAAI,QAAQ,IAAI,gBAAgB;AAC9B,WAAO,SAAS,EAAE,GAAG,OAAO,QAAQ,SAAS,MAAM,QAAQ,QAAQ,IAAI,eAAe;AAAA,EACxF;AAGA,MAAI,QAAQ,IAAI,oBAAoB;AAClC,WAAO,aAAa,EAAE,GAAG,OAAO,YAAY,SAAS,MAAM,QAAQ,QAAQ,IAAI,mBAAmB;AAAA,EACpG;AAGA,MAAI,QAAQ,IAAI,gBAAgB;AAC9B,WAAO,SAAS,EAAE,GAAG,OAAO,QAAQ,SAAS,MAAM,QAAQ,QAAQ,IAAI,eAAe;AAAA,EACxF;AAEA,SAAO;AACT;AAKA,SAAS,gBAAgB,UAA2C;AAClE,QAAM,iBAAgC;AAAA,IACpC;AAAA,IAAc;AAAA,IAAU;AAAA,IAAa;AAAA,IAAU;AAAA,IAC/C;AAAA,IAAY;AAAA,IAAY;AAAA,IAAY;AAAA,IAAS;AAAA,IAC7C;AAAA,IAAY;AAAA,IAAa;AAAA,IAAW;AAAA,IAAU;AAAA,IAC9C;AAAA,EACF;AACA,SAAO,eAAe,SAAS,QAAuB;AACxD;AAKO,SAAS,qBAAoC;AAClD,SAAO;AAAA,IACL,YAAY,EAAE,QAAQ,QAAQ,IAAI,mBAAmB;AAAA,IACrD,QAAQ,EAAE,QAAQ,QAAQ,IAAI,eAAe;AAAA,IAC7C,WAAW,EAAE,QAAQ,QAAQ,IAAI,kBAAkB;AAAA,IACnD,QAAQ,EAAE,QAAQ,QAAQ,IAAI,eAAe;AAAA,IAC7C,KAAK,EAAE,QAAQ,QAAQ,IAAI,YAAY;AAAA,IACvC,UAAU,EAAE,QAAQ,QAAQ,IAAI,iBAAiB;AAAA,IACjD,UAAU,EAAE,QAAQ,QAAQ,IAAI,iBAAiB;AAAA,IACjD,UAAU,EAAE,QAAQ,QAAQ,IAAI,iBAAiB;AAAA,IACjD,OAAO,EAAE,QAAQ,QAAQ,IAAI,sBAAsB,UAAU,QAAQ,IAAI,sBAAsB;AAAA,IAC/F,UAAU,EAAE,QAAQ,QAAQ,IAAI,iBAAiB;AAAA,IACjD,UAAU,EAAE,QAAQ,QAAQ,IAAI,iBAAiB;AAAA,IACjD,WAAW,EAAE,QAAQ,QAAQ,IAAI,kBAAkB;AAAA,IACnD,SAAS,EAAE,QAAQ,QAAQ,IAAI,gBAAgB;AAAA,IAC/C,QAAQ,EAAE,QAAQ,QAAQ,IAAI,eAAe;AAAA,IAC7C,YAAY,EAAE,QAAQ,QAAQ,IAAI,mBAAmB;AAAA,IACrD,QAAQ,EAAE,QAAQ,QAAQ,IAAI,eAAe;AAAA,EAC/C;AACF;;;AChNA,eAAsB,SAAS,MAAqC;AAElE,SAAO,MAAM,KAAK,EAAE,QAAQ,KAAK,GAAG,CAAC,GAAG,OAAO;AAAA,IAC7C,MAAM,IAAI,KAAK,KAAK,IAAI,IAAI,IAAI,KAAK,KAAK,KAAK,GAAI,EAAE,YAAY,EAAE,MAAM,GAAG,EAAE,CAAC;AAAA,IAC/E,UAAU;AAAA,IACV,cAAc;AAAA,IACd,kBAAkB;AAAA,IAClB,MAAM;AAAA,EACR,EAAE;AACJ;AAKO,SAAS,iBAAiB,OAA6B;AAC5D,MAAI,MAAM,WAAW,EAAG,QAAO;AAE/B,QAAM,QAAQ;AAAA,IACZ;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAEA,aAAW,OAAO,OAAO;AACvB,UAAM,SAAS,IAAI,eAAe,IAAI;AACtC,UAAM;AAAA,MACJ,GAAG,IAAI,IAAI,MAAM,IAAI,SAAS,SAAS,EAAE,SAAS,CAAC,CAAC,MAAM,OAAO,SAAS,EAAE,SAAS,CAAC,CAAC,OAAO,IAAI,KAAK,QAAQ,CAAC,CAAC;AAAA,IACnH;AAAA,EACF;AAEA,SAAO,MAAM,KAAK,IAAI;AACxB;;;AC5EA,SAAS,oBAA+D;AAOxE,IAAMA,gBAAe;AACrB,IAAM,0BAA0B;AAGhC,IAAM,gBAA6C;AAAA,EACjD,YAAY;AAAA,EACZ,QAAQ;AAAA,EACR,WAAW;AAAA,EACX,QAAQ;AAAA,EACR,KAAK;AAAA,EACL,UAAU;AAAA,EACV,UAAU;AAAA,EACV,UAAU;AAAA,EACV,OAAO;AAAA,EACP,UAAU;AAAA,EACV,UAAU;AAAA,EACV,WAAW;AAAA,EACX,SAAS;AAAA,EACT,QAAQ;AAAA,EACR,YAAY;AAAA,EACZ,QAAQ;AAAA,EACR,QAAQ;AACV;AAGA,IAAM,oBAAiD;AAAA,EACrD,YAAY;AAAA,EACZ,QAAQ;AAAA,EACR,WAAW;AAAA,EACX,QAAQ;AAAA,EACR,KAAK;AAAA,EACL,UAAU;AAAA,EACV,UAAU;AAAA,EACV,UAAU;AAAA,EACV,OAAO;AAAA,EACP,UAAU;AAAA,EACV,UAAU;AAAA,EACV,WAAW;AAAA,EACX,SAAS;AAAA,EACT,QAAQ;AAAA,EACR,YAAY;AAAA,EACZ,QAAQ;AAAA,EACR,QAAQ;AACV;AAkBO,SAASC,gBAAuB;AACrC,QAAM,UAAU,QAAQ,IAAI;AAC5B,MAAI,SAAS;AACX,UAAM,SAAS,SAAS,SAAS,EAAE;AACnC,QAAI,CAAC,MAAM,MAAM,KAAK,SAAS,KAAK,SAAS,MAAO,QAAO;AAAA,EAC7D;AACA,SAAOD;AACT;AAKA,eAAe,mBAAmB,MAAgC;AAChE,QAAM,aAAa,IAAI,gBAAgB;AACvC,QAAM,YAAY,WAAW,MAAM,WAAW,MAAM,GAAG,uBAAuB;AAC9E,MAAI;AACF,UAAM,WAAW,MAAM,MAAM,oBAAoB,IAAI,WAAW;AAAA,MAC9D,QAAQ,WAAW;AAAA,IACrB,CAAC;AACD,iBAAa,SAAS;AACtB,QAAI,SAAS,IAAI;AACf,YAAM,OAAQ,MAAM,SAAS,KAAK;AAClC,aAAO,KAAK,WAAW;AAAA,IACzB;AACA,WAAO;AAAA,EACT,QAAQ;AACN,iBAAa,SAAS;AACtB,WAAO;AAAA,EACT;AACF;AAKA,SAAS,gBAAgB,SAAwE;AAC/F,QAAM,UAAU,QAAQ,UAAU,IAAI,CAAC,MAAW,EAAE,OAAO,EAAE,KAAK,GAAG,EAAE,YAAY,KAAK;AAGxF,MAAI,4FAA4F,KAAK,OAAO,GAAG;AAC7G,WAAO;AAAA,EACT;AAGA,MAAI,QAAQ,UAAU,KAAK,CAAC,MAAW,OAAO,EAAE,YAAY,YAAY,MAAM,QAAQ,EAAE,OAAO,KAAK,EAAE,QAAQ,KAAK,CAAC,MAAW,EAAE,SAAS,WAAW,CAAC,GAAG;AACvJ,WAAO;AAAA,EACT;AAGA,MAAI,QAAQ,SAAS,IAAK,QAAO;AACjC,MAAI,QAAQ,SAAS,IAAM,QAAO;AAClC,SAAO;AACT;AA8FA,SAAS,oBAAoB,MAAkF;AAE7G,QAAM,aAA+F;AAAA,IACnG,QAAQ;AAAA;AAAA,MAEN,EAAE,UAAU,YAAY,OAAO,cAAc,QAAQ,MAAM;AAAA,MAC3D,EAAE,UAAU,cAAc,OAAO,mBAAmB,QAAQ,MAAM;AAAA;AAAA,MAElE,EAAE,UAAU,YAAY,OAAO,kBAAkB,QAAQ,MAAM;AAAA,MAC/D,EAAE,UAAU,YAAY,OAAO,cAAc,QAAQ,MAAM;AAAA,MAC3D,EAAE,UAAU,YAAY,OAAO,qBAAqB,QAAQ,MAAM;AAAA;AAAA,MAElE,EAAE,UAAU,UAAU,OAAO,wBAAwB,QAAQ,MAAM;AAAA,MACnE,EAAE,UAAU,UAAU,OAAO,0CAA0C,QAAQ,MAAM;AAAA,MACrF,EAAE,UAAU,UAAU,OAAO,wBAAwB,QAAQ,MAAM;AAAA,IACrE;AAAA,IACA,QAAQ;AAAA;AAAA,MAEN,EAAE,UAAU,UAAU,OAAO,qBAAqB,QAAQ,MAAM;AAAA,MAChE,EAAE,UAAU,UAAU,OAAO,4BAA4B,QAAQ,MAAM;AAAA;AAAA,MAEvE,EAAE,UAAU,YAAY,OAAO,cAAc,QAAQ,MAAM;AAAA;AAAA,MAE3D,EAAE,UAAU,cAAc,OAAO,gBAAgB,QAAQ,MAAM;AAAA,MAC/D,EAAE,UAAU,cAAc,OAAO,oBAAoB,QAAQ,MAAM;AAAA;AAAA,MAEnE,EAAE,UAAU,UAAU,OAAO,0CAA0C,QAAQ,MAAM;AAAA,MACrF,EAAE,UAAU,UAAU,OAAO,wBAAwB,QAAQ,MAAM;AAAA,IACrE;AAAA,IACA,SAAS;AAAA;AAAA,MAEP,EAAE,UAAU,UAAU,OAAO,qBAAqB,QAAQ,MAAM;AAAA,MAChE,EAAE,UAAU,UAAU,OAAO,4BAA4B,QAAQ,MAAM;AAAA;AAAA,MAEvE,EAAE,UAAU,cAAc,OAAO,gBAAgB,QAAQ,MAAM;AAAA;AAAA,MAE/D,EAAE,UAAU,UAAU,OAAO,wBAAwB,QAAQ,MAAM;AAAA;AAAA,MAEnE,EAAE,UAAU,cAAc,OAAO,6BAA6B,QAAQ,KAAK;AAAA,MAC3E,EAAE,UAAU,cAAc,OAAO,yBAAyB,QAAQ,KAAK;AAAA,MACvE,EAAE,UAAU,cAAc,OAAO,iBAAiB,QAAQ,KAAK;AAAA,IACjE;AAAA,IACA,WAAW;AAAA;AAAA,MAET,EAAE,UAAU,UAAU,OAAO,4BAA4B,QAAQ,MAAM;AAAA;AAAA,MAEvE,EAAE,UAAU,cAAc,OAAO,kBAAkB,QAAQ,KAAK;AAAA;AAAA,MAEhE,EAAE,UAAU,cAAc,OAAO,2BAA2B,QAAQ,KAAK;AAAA,IAC3E;AAAA,IACA,QAAQ;AAAA,MACN,EAAE,UAAU,YAAY,OAAO,cAAc,QAAQ,MAAM;AAAA,MAC3D,EAAE,UAAU,cAAc,OAAO,2BAA2B,QAAQ,MAAM;AAAA,MAC1E,EAAE,UAAU,cAAc,OAAO,6BAA6B,QAAQ,KAAK;AAAA,MAC3E,EAAE,UAAU,cAAc,OAAO,iBAAiB,QAAQ,KAAK;AAAA,IACjE;AAAA,EACF;AAEA,QAAM,YAAY,WAAW,IAAI,KAAK,WAAW;AAGjD,SAAO,UAAU,OAAO,CAAC,EAAE,SAAS,MAAM;AACxC,UAAM,SAAS,UAAU,QAAQ;AACjC,WAAO,CAAC,CAAC;AAAA,EACX,CAAC;AACH;AAMA,eAAe,4BACb,KACA,KACA,SACA,UACA,SACA,QAC+B;AAC/B,MAAI;AACF,UAAM,UAAU,cAAc,QAAQ;AACtC,UAAM,kBAAkB,qBAAqB,SAAS,UAAU,OAAO;AAEvE,YAAQ,IAAI,wBAAwB,QAAQ,IAAI,OAAO,EAAE;AAEzD,UAAM,WAAW,MAAM,MAAM,GAAG,OAAO,qBAAqB;AAAA,MAC1D,QAAQ;AAAA,MACR,SAAS;AAAA,QACP,gBAAgB;AAAA,QAChB,iBAAiB,UAAU,MAAM;AAAA,QACjC,GAAI,aAAa,eAAe,EAAE,gBAAgB,yBAAyB,WAAW,UAAU,IAAI,CAAC;AAAA,MACvG;AAAA,MACA,MAAM,KAAK,UAAU,eAAe;AAAA,IACtC,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,YAAY,MAAM,SAAS,KAAK;AACtC,YAAM,cAAc,SAAS,WAAW;AACxC,YAAM,gBAAgB,SAAS,UAAU;AAEzC,cAAQ,MAAM,aAAa,QAAQ,IAAI,OAAO,YAAY,SAAS,MAAM,IAAI,cAAc,oBAAoB,EAAE,KAAK,UAAU,UAAU,GAAG,GAAG,CAAC,EAAE;AAGnJ,UAAI,eAAe,eAAe;AAChC,eAAO,EAAE,SAAS,MAAM;AAAA,MAC1B;AAIA,UAAI,CAAC,IAAI,aAAa;AACpB,YAAI,UAAU,SAAS,QAAQ,EAAE,gBAAgB,mBAAmB,CAAC;AACrE,YAAI,IAAI,KAAK,UAAU;AAAA,UACrB,OAAO,mBAAmB,QAAQ,MAAM,SAAS;AAAA,UACjD;AAAA,UACA,OAAO;AAAA,QACT,CAAC,CAAC;AAAA,MACJ;AACA,aAAO,EAAE,SAAS,MAAM;AAAA,IAC1B;AAGA,UAAM,cAAc,QAAQ,WAAW;AAEvC,QAAI,eAAe,SAAS,MAAM;AAChC,UAAI,UAAU,KAAK;AAAA,QACjB,gBAAgB;AAAA,QAChB,iBAAiB;AAAA,QACjB,cAAc;AAAA,MAChB,CAAC;AAED,YAAM,SAAS,SAAS,KAAK,UAAU;AAEvC,UAAI;AACF,eAAO,MAAM;AACX,gBAAM,EAAE,MAAM,MAAM,IAAI,MAAM,OAAO,KAAK;AAC1C,cAAI,KAAM;AACV,cAAI,MAAM,KAAK;AAAA,QACjB;AAAA,MACF,UAAE;AACA,YAAI,IAAI;AAAA,MACV;AAAA,IACF,OAAO;AACL,YAAM,OAAO,MAAM,SAAS,KAAK;AACjC,UAAI,UAAU,KAAK,EAAE,gBAAgB,mBAAmB,CAAC;AACzD,UAAI,IAAI,IAAI;AAAA,IACd;AAEA,WAAO,EAAE,SAAS,KAAK;AAAA,EAEzB,SAAS,OAAO;AACd,YAAQ,MAAM,kCAAkC,QAAQ,IAAI,OAAO,KAAK,KAAK;AAC7E,WAAO,EAAE,SAAS,MAAM;AAAA,EAC1B;AACF;AAKA,SAAS,4BAA+E;AAEtF,QAAM,WAA0B;AAAA,IAC9B;AAAA;AAAA,IACA;AAAA;AAAA,IACA;AAAA;AAAA,IACA;AAAA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAEA,aAAW,YAAY,UAAU;AAC/B,UAAM,SAAS,UAAU,QAAQ;AACjC,QAAI,QAAQ;AAEV,UAAI,UAAU;AACd,UAAI,aAAa,WAAY,WAAU;AAAA,eAC9B,aAAa,SAAU,WAAU;AAAA,eACjC,aAAa,SAAU,WAAU;AAAA,eACjC,aAAa,aAAc,WAAU;AAAA,eACrC,aAAa,YAAa,WAAU;AAAA,eACpC,aAAa,SAAU,WAAU;AAAA,eACjC,aAAa,MAAO,WAAU;AAAA,eAC9B,aAAa,WAAY,WAAU;AAAA,eACnC,aAAa,WAAY,WAAU;AAE5C,aAAO,EAAE,UAAU,QAAQ;AAAA,IAC7B;AAAA,EACF;AAEA,SAAO;AACT;AAKA,SAAS,gBAAgB,OAAkE;AAEzF,MAAI,MAAM,WAAW,WAAW,GAAG;AACjC,YAAQ,MAAM,QAAQ,aAAa,EAAE;AAAA,EACvC;AAGA,MAAI,UAAU,UAAU,UAAU,iBAAiB;AAGjD,WAAO,EAAE,UAAU,YAAY,SAAS,aAAa;AAAA,EACvD;AAGA,QAAM,kBAAkB,kBAAkB,KAAK;AAG/C,QAAM,YAAY,SAAS,eAAe;AAC1C,MAAI,WAAW;AACb,WAAO,EAAE,UAAU,UAAU,UAAU,SAAS,UAAU,GAAG;AAAA,EAC/D;AAGA,aAAW,YAAY,OAAO,KAAK,aAAa,GAAoB;AAClE,QAAI,gBAAgB,WAAW,QAAQ,KAAK,gBAAgB,WAAW,GAAG,QAAQ,GAAG,GAAG;AACtF,aAAO;AAAA,QACL;AAAA,QACA,SAAS,gBAAgB,QAAQ,GAAG,QAAQ,KAAK,EAAE;AAAA,MACrD;AAAA,IACF;AAAA,EACF;AAGA,QAAM,YAAY,0BAA0B;AAC5C,MAAI,WAAW;AACb,WAAO;AAAA,EACT;AAGA,SAAO,EAAE,UAAU,cAAc,SAAS,gBAAgB;AAC5D;AAKA,SAAS,UAAU,UAA2C;AAC5D,QAAM,SAAS,kBAAkB,QAAQ;AACzC,SAAO,QAAQ,IAAI,MAAM;AAC3B;AAKA,eAAe,sBACb,KACA,KACA,MACe;AACf,MAAI;AACF,UAAM,UAAU,KAAK,MAAM,IAAI;AAC/B,UAAM,QAAQ,QAAQ,SAAS;AAE/B,YAAQ,IAAI,gCAAgC,KAAK,EAAE;AAGnD,QAAI,UAAU,UAAU,UAAU,iBAAiB;AACjD,YAAM,OAAO,gBAAgB,OAAO;AACpC,YAAM,SAAS,oBAAoB,IAAI;AAEvC,UAAI,OAAO,WAAW,GAAG;AACvB,YAAI,UAAU,KAAK,EAAE,gBAAgB,mBAAmB,CAAC;AACzD,YAAI,IAAI,KAAK,UAAU,EAAE,OAAO,yBAAyB,CAAC,CAAC;AAC3D;AAAA,MACF;AAEA,cAAQ,IAAI,iCAAiC,IAAI,YAAY,OAAO,MAAM,SAAS;AAGnF,eAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK;AACtC,cAAM,EAAE,UAAAE,WAAU,OAAOC,UAAS,OAAO,IAAI,OAAO,CAAC;AACrD,cAAMC,UAAS,UAAUF,SAAQ;AAEjC,YAAI,CAACE,SAAQ;AACX,kBAAQ,IAAI,sBAAsBF,SAAQ,IAAIC,QAAO,eAAe;AACpE;AAAA,QACF;AAEA,gBAAQ,IAAI,qBAAqB,IAAI,CAAC,IAAI,OAAO,MAAM,KAAKD,SAAQ,IAAIC,QAAO,KAAK,SAAS,SAAS,MAAM,GAAG;AAE/G,cAAM,SAAS,MAAM,4BAA4B,KAAK,KAAK,SAASD,WAAUC,UAASC,OAAM;AAE7F,YAAI,OAAO,SAAS;AAClB,kBAAQ,IAAI,0BAA0BF,SAAQ,IAAIC,QAAO,EAAE;AAC3D;AAAA,QACF;AAGA,YAAI,MAAM,OAAO,SAAS,GAAG;AAC3B,kBAAQ,IAAI,iBAAiB,OAAO,MAAM,gBAAgB;AAC1D;AAAA,QACF;AAEA,gBAAQ,IAAI,aAAaD,SAAQ,IAAIC,QAAO,8BAA8B;AAAA,MAK5E;AAEA;AAAA,IACF;AAGA,UAAM,WAAW,gBAAgB,KAAK;AACtC,QAAI,CAAC,UAAU;AACb,UAAI,UAAU,KAAK,EAAE,gBAAgB,mBAAmB,CAAC;AACzD,UAAI,IAAI,KAAK,UAAU,EAAE,OAAO,0BAA0B,CAAC,CAAC;AAC5D;AAAA,IACF;AAEA,UAAM,EAAE,UAAU,QAAQ,IAAI;AAC9B,UAAM,SAAS,UAAU,QAAQ;AAEjC,QAAI,CAAC,QAAQ;AACX,YAAM,gBAAgB,UAAU,YAAY;AAC5C,UAAI,iBAAiB,aAAa,cAAc;AAC9C,gBAAQ,IAAI,4BAA4B,QAAQ,8BAA8B;AAC9E,cAAM,kBAAkB,KAAK,KAAK,SAAS,SAAS,aAAa;AACjE;AAAA,MACF;AAEA,UAAI,UAAU,KAAK,EAAE,gBAAgB,mBAAmB,CAAC;AACzD,UAAI,IAAI,KAAK,UAAU;AAAA,QACrB,OAAO,uCAAuC,QAAQ,SAAS,kBAAkB,QAAQ,CAAC;AAAA,MAC5F,CAAC,CAAC;AACF;AAAA,IACF;AAEA,UAAM,gBAAgB,KAAK,KAAK,SAAS,UAAU,SAAS,MAAM;AAAA,EAEpE,SAAS,OAAO;AACd,YAAQ,MAAM,qCAAqC,KAAK;AACxD,QAAI,UAAU,KAAK,EAAE,gBAAgB,mBAAmB,CAAC;AACzD,QAAI,IAAI,KAAK,UAAU;AAAA,MACrB,OAAO,iBAAiB,QAAQ,MAAM,UAAU;AAAA,IAClD,CAAC,CAAC;AAAA,EACJ;AACF;AAKA,eAAe,gBACb,KACA,KACA,SACA,UACA,SACA,QACe;AACf,QAAM,UAAU,cAAc,QAAQ;AAGtC,QAAM,kBAAkB,qBAAqB,SAAS,UAAU,OAAO;AAEvE,UAAQ,IAAI,wBAAwB,QAAQ,eAAe,OAAO,EAAE;AAGpE,QAAM,WAAW,MAAM,MAAM,GAAG,OAAO,qBAAqB;AAAA,IAC1D,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,gBAAgB;AAAA,MAChB,iBAAiB,UAAU,MAAM;AAAA,MACjC,GAAI,aAAa,eAAe,EAAE,gBAAgB,yBAAyB,WAAW,UAAU,IAAI,CAAC;AAAA,IACvG;AAAA,IACA,MAAM,KAAK,UAAU,eAAe;AAAA,EACtC,CAAC;AAED,MAAI,CAAC,SAAS,IAAI;AAChB,UAAM,QAAQ,MAAM,SAAS,KAAK;AAClC,YAAQ,MAAM,6BAA6B,SAAS,MAAM,MAAM,KAAK;AACrE,QAAI,UAAU,SAAS,QAAQ,EAAE,gBAAgB,mBAAmB,CAAC;AACrE,QAAI,IAAI,KAAK,UAAU,EAAE,OAAO,mBAAmB,KAAK,GAAG,CAAC,CAAC;AAC7D;AAAA,EACF;AAGA,QAAM,cAAc,QAAQ,WAAW;AAEvC,MAAI,eAAe,SAAS,MAAM;AAChC,QAAI,UAAU,KAAK;AAAA,MACjB,gBAAgB;AAAA,MAChB,iBAAiB;AAAA,MACjB,cAAc;AAAA,IAChB,CAAC;AAED,UAAM,SAAS,SAAS,KAAK,UAAU;AAEvC,QAAI;AACF,aAAO,MAAM;AACX,cAAM,EAAE,MAAM,MAAM,IAAI,MAAM,OAAO,KAAK;AAC1C,YAAI,KAAM;AACV,YAAI,MAAM,KAAK;AAAA,MACjB;AAAA,IACF,UAAE;AACA,UAAI,IAAI;AAAA,IACV;AAAA,EACF,OAAO;AACL,UAAM,OAAO,MAAM,SAAS,KAAK;AACjC,QAAI,UAAU,KAAK,EAAE,gBAAgB,mBAAmB,CAAC;AACzD,QAAI,IAAI,IAAI;AAAA,EACd;AACF;AAKA,eAAe,kBACb,KACA,KACA,SACA,SACA,QACe;AACf,QAAM,kBAAkB,QAAQ,SAAS,GAAG,IAAI,UAAU,GAAG,qBAAqB,OAAO,CAAC,IAAI,OAAO;AAErG,UAAQ,IAAI,yCAAyC,eAAe,EAAE;AAEtE,QAAM,WAAW,MAAM,MAAM,iDAAiD;AAAA,IAC5E,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,gBAAgB;AAAA,MAChB,iBAAiB,UAAU,MAAM;AAAA,MACjC,gBAAgB;AAAA,MAChB,WAAW;AAAA,IACb;AAAA,IACA,MAAM,KAAK,UAAU;AAAA,MACnB,GAAG;AAAA,MACH,OAAO;AAAA,IACT,CAAC;AAAA,EACH,CAAC;AAED,MAAI,CAAC,SAAS,IAAI;AAChB,UAAM,QAAQ,MAAM,SAAS,KAAK;AAClC,YAAQ,MAAM,+BAA+B,SAAS,MAAM,MAAM,KAAK;AACvE,QAAI,UAAU,SAAS,QAAQ,EAAE,gBAAgB,mBAAmB,CAAC;AACrE,QAAI,IAAI,KAAK,UAAU,EAAE,OAAO,qBAAqB,KAAK,GAAG,CAAC,CAAC;AAC/D;AAAA,EACF;AAEA,QAAM,cAAc,QAAQ,WAAW;AAEvC,MAAI,eAAe,SAAS,MAAM;AAChC,QAAI,UAAU,KAAK;AAAA,MACjB,gBAAgB;AAAA,MAChB,iBAAiB;AAAA,MACjB,cAAc;AAAA,IAChB,CAAC;AAED,UAAM,SAAS,SAAS,KAAK,UAAU;AAEvC,QAAI;AACF,aAAO,MAAM;AACX,cAAM,EAAE,MAAM,MAAM,IAAI,MAAM,OAAO,KAAK;AAC1C,YAAI,KAAM;AACV,YAAI,MAAM,KAAK;AAAA,MACjB;AAAA,IACF,UAAE;AACA,UAAI,IAAI;AAAA,IACV;AAAA,EACF,OAAO;AACL,UAAM,OAAO,MAAM,SAAS,KAAK;AACjC,QAAI,UAAU,KAAK,EAAE,gBAAgB,mBAAmB,CAAC;AACzD,QAAI,IAAI,IAAI;AAAA,EACd;AACF;AAKA,SAAS,qBACP,SACA,UACA,SACyB;AACzB,QAAM,cAAc;AAAA,IAClB,OAAO;AAAA,IACP,UAAU,QAAQ;AAAA,IAClB,aAAa,QAAQ;AAAA,IACrB,YAAY,QAAQ,cAAc,QAAQ;AAAA,IAC1C,OAAO,QAAQ,SAAS,QAAQ;AAAA,IAChC,QAAQ,QAAQ;AAAA;AAAA,IAEhB,GAAI,QAAQ,SAAS,EAAE,OAAO,QAAQ,MAAM;AAAA,IAC5C,GAAI,QAAQ,eAAe,EAAE,aAAa,QAAQ,YAAY;AAAA,EAChE;AAGA,UAAQ,UAAU;AAAA,IAChB,KAAK;AAEH,aAAO;AAAA,QACL,OAAO;AAAA,QACP,UAAU,QAAQ;AAAA,QAClB,YAAY,QAAQ,cAAc,QAAQ,aAAa;AAAA,QACvD,aAAa,QAAQ;AAAA,QACrB,OAAO,QAAQ,SAAS,QAAQ;AAAA,QAChC,QAAQ,QAAQ;AAAA;AAAA,QAEhB,GAAI,QAAQ,SAAS,EAAE,OAAO,QAAQ,MAAM;AAAA,QAC5C,GAAI,QAAQ,eAAe,EAAE,aAAa,QAAQ,YAAY;AAAA,MAChE;AAAA,IACF,KAAK;AACH,aAAO;AAAA,QACL,OAAO;AAAA,QACP,UAAU,QAAQ;AAAA,QAClB,YAAY,QAAQ,cAAc,QAAQ,aAAa;AAAA,QACvD,aAAa,QAAQ;AAAA,QACrB,OAAO,QAAQ,SAAS,QAAQ;AAAA,QAChC,QAAQ,QAAQ;AAAA;AAAA,QAEhB,GAAI,QAAQ,SAAS,EAAE,OAAO,QAAQ,MAAM;AAAA,QAC5C,GAAI,QAAQ,eAAe,EAAE,aAAa,QAAQ,YAAY;AAAA,MAChE;AAAA,IACF,KAAK;AACH,aAAO;AAAA,QACL,OAAO;AAAA,QACP,UAAW,QAAQ,SAAoD,IAAI,QAAM;AAAA,UAC/E,MAAM,EAAE,SAAS,cAAc,UAAU,EAAE;AAAA,UAC3C,OAAO,CAAC,EAAE,MAAM,EAAE,QAAQ,CAAC;AAAA,QAC7B,EAAE;AAAA,QACF,kBAAkB;AAAA,UAChB,aAAa,QAAQ;AAAA,UACrB,iBAAiB,QAAQ,cAAc,QAAQ;AAAA,UAC/C,MAAM,QAAQ,SAAS,QAAQ;AAAA,QACjC;AAAA,MACF;AAAA,IACF;AACE,aAAO;AAAA,EACX;AACF;AAKA,SAAS,qBAAqB,SAAyB;AACrD,QAAM,WAAmC;AAAA,IACvC,OAAO;AAAA,IACP,UAAU;AAAA,IACV,UAAU;AAAA,IACV,QAAQ;AAAA,IACR,YAAY;AAAA,IACZ,QAAQ;AAAA,IACR,cAAc;AAAA,IACd,QAAQ;AAAA,EACV;AAEA,aAAW,CAAC,QAAQ,QAAQ,KAAK,OAAO,QAAQ,QAAQ,GAAG;AACzD,QAAI,QAAQ,YAAY,EAAE,SAAS,MAAM,GAAG;AAC1C,aAAO;AAAA,IACT;AAAA,EACF;AAEA,SAAO;AACT;AAKA,eAAe,SAAS,KAAuC;AAC7D,SAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AACtC,QAAI,OAAO;AACX,QAAI,GAAG,QAAQ,CAAC,UAAU;AACxB,cAAQ,MAAM,SAAS;AAAA,IACzB,CAAC;AACD,QAAI,GAAG,OAAO,MAAM,QAAQ,IAAI,CAAC;AACjC,QAAI,GAAG,SAAS,MAAM;AAAA,EACxB,CAAC;AACH;AAKA,eAAsB,WAAW,UAAwB,CAAC,GAAyB;AACjF,QAAM,OAAO,QAAQ,QAAQE,cAAa;AAG1C,QAAM,WAAW,MAAM,mBAAmB,IAAI;AAC9C,MAAI,UAAU;AACZ,YAAQ,IAAI,2CAA2C,IAAI,EAAE;AAC7D,WAAO;AAAA,MACL;AAAA,MACA,SAAS,oBAAoB,IAAI;AAAA,MACjC,OAAO,YAAY;AAAA,MAAC;AAAA,IACtB;AAAA,EACF;AAEA,SAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AACtC,UAAM,SAAS,aAAa,OAAO,KAAK,QAAQ;AAE9C,UAAI,UAAU,+BAA+B,GAAG;AAChD,UAAI,UAAU,gCAAgC,oBAAoB;AAClE,UAAI,UAAU,gCAAgC,6BAA6B;AAE3E,UAAI,IAAI,WAAW,WAAW;AAC5B,YAAI,UAAU,GAAG;AACjB,YAAI,IAAI;AACR;AAAA,MACF;AAGA,UAAI,IAAI,QAAQ,aAAa,IAAI,WAAW,OAAO;AACjD,YAAI,UAAU,KAAK,EAAE,gBAAgB,mBAAmB,CAAC;AACzD,YAAI,IAAI,KAAK,UAAU,EAAE,QAAQ,MAAM,UAAU,WAAW,CAAC,CAAC;AAC9D;AAAA,MACF;AAGA,UAAI,IAAI,QAAQ,gBAAgB,IAAI,WAAW,OAAO;AACpD,YAAI,UAAU,KAAK,EAAE,gBAAgB,mBAAmB,CAAC;AACzD,YAAI,IAAI,KAAK,UAAU;AAAA,UACrB,QAAQ;AAAA,UACR,MAAM,OAAO,KAAK,QAAQ,EAAE,IAAI,CAAC,QAAQ;AAAA,YACvC;AAAA,YACA,QAAQ;AAAA,YACR,SAAS,KAAK,IAAI;AAAA,YAClB,UAAU;AAAA,UACZ,EAAE;AAAA,QACJ,CAAC,CAAC;AACF;AAAA,MACF;AAGA,UAAI,IAAI,QAAQ,0BAA0B,IAAI,WAAW,QAAQ;AAC/D,YAAI;AACF,gBAAM,OAAO,MAAM,SAAS,GAAG;AAC/B,gBAAM,sBAAsB,KAAK,KAAK,IAAI;AAAA,QAC5C,SAAS,OAAO;AACd,kBAAQ,MAAM,oBAAoB,KAAK;AACvC,cAAI,UAAU,KAAK,EAAE,gBAAgB,mBAAmB,CAAC;AACzD,cAAI,IAAI,KAAK,UAAU,EAAE,OAAO,wBAAwB,CAAC,CAAC;AAAA,QAC5D;AACA;AAAA,MACF;AAGA,UAAI,UAAU,KAAK,EAAE,gBAAgB,mBAAmB,CAAC;AACzD,UAAI,IAAI,KAAK,UAAU,EAAE,OAAO,YAAY,CAAC,CAAC;AAAA,IAChD,CAAC;AAED,WAAO,OAAO,MAAM,aAAa,MAAM;AACrC,YAAM,UAAU,OAAO,QAAQ;AAC/B,cAAQ,IAAI,qCAAqC,QAAQ,IAAI,EAAE;AAE/D,UAAI,QAAQ,SAAS;AACnB,gBAAQ,QAAQ,QAAQ,IAAI;AAAA,MAC9B;AAEA,cAAQ;AAAA,QACN,MAAM,QAAQ;AAAA,QACd,SAAS,oBAAoB,QAAQ,IAAI;AAAA,QACzC,OAAO,MAAM;AACX,iBAAO,IAAI,QAAQ,CAAC,QAAQ;AAC1B,mBAAO,MAAM,MAAM,IAAI,CAAC;AAAA,UAC1B,CAAC;AAAA,QACH;AAAA,MACF,CAAC;AAAA,IACH,CAAC;AAED,WAAO,GAAG,SAAS,CAAC,UAAU;AAC5B,cAAQ,MAAM,2BAA2B,KAAK;AAC9C,UAAI,QAAQ,SAAS;AACnB,gBAAQ,QAAQ,KAAK;AAAA,MACvB;AACA,aAAO,KAAK;AAAA,IACd,CAAC;AAAA,EACH,CAAC;AACH;;;ACv2BA,SAAS,cAAc,eAAe,YAAY,iBAAiB;AACnE,SAAS,eAAe;AACxB,SAAS,YAAY;AAGrB,IAAI,oBAAwC;AAK5C,eAAe,iBAAiB,KAAuC;AACrE,MAAI;AACF,UAAM,QAAQ,MAAM,WAAW;AAAA,MAC7B,MAAM,aAAa;AAAA,MACnB,SAAS,CAAC,SAAS;AACjB,YAAI,OAAO,KAAK,mCAAmC,IAAI,EAAE;AAAA,MAC3D;AAAA,MACA,SAAS,CAAC,UAAU;AAClB,YAAI,OAAO,MAAM,wBAAwB,MAAM,OAAO,EAAE;AAAA,MAC1D;AAAA,IACF,CAAC;AAED,wBAAoB;AAGpB,UAAM,UAAU,MAAM,mBAAmB,MAAM,MAAM,GAAI;AACzD,QAAI,CAAC,SAAS;AACZ,UAAI,OAAO,KAAK,sCAAsC;AAAA,IACxD;AAAA,EACF,SAAS,KAAK;AACZ,QAAI,OAAO,MAAM,kCAAkC,eAAe,QAAQ,IAAI,UAAU,OAAO,GAAG,CAAC,EAAE;AACrG,UAAM;AAAA,EACR;AACF;AAKA,eAAe,mBAAmB,MAAc,YAAY,KAAwB;AAClF,QAAM,QAAQ,KAAK,IAAI;AACvB,SAAO,KAAK,IAAI,IAAI,QAAQ,WAAW;AACrC,QAAI;AACF,YAAM,MAAM,MAAM,MAAM,oBAAoB,IAAI,SAAS;AACzD,UAAI,IAAI,GAAI,QAAO;AAAA,IACrB,QAAQ;AAAA,IAAkB;AAC1B,UAAM,IAAI,QAAQ,CAAC,MAAM,WAAW,GAAG,GAAG,CAAC;AAAA,EAC7C;AACA,SAAO;AACT;AAKA,SAAS,mBAA4B;AACnC,SAAO,QAAQ,KAAK,KAAK,CAAC,KAAK,MAAM,QAAQ,gBAAgB,KAAK,KAAK,KAAK,CAAC;AAC/E;AAMA,SAAS,mBAAmB,QAA+C;AACzE,QAAM,YAAY,KAAK,QAAQ,GAAG,WAAW;AAC7C,QAAM,aAAa,KAAK,WAAW,eAAe;AAElD,MAAI,SAAkC,CAAC;AACvC,MAAI,aAAa;AAEjB,MAAI,CAAC,WAAW,SAAS,GAAG;AAC1B,QAAI;AACF,gBAAU,WAAW,EAAE,WAAW,KAAK,CAAC;AAAA,IAC1C,QAAQ;AACN;AAAA,IACF;AAAA,EACF;AAEA,MAAI,WAAW,UAAU,GAAG;AAC1B,QAAI;AACF,YAAM,UAAU,aAAa,YAAY,OAAO,EAAE,KAAK;AACvD,UAAI,QAAS,UAAS,KAAK,MAAM,OAAO;AAAA,UACnC,cAAa;AAAA,IACpB,QAAQ;AACN,eAAS,CAAC;AACV,mBAAa;AAAA,IACf;AAAA,EACF,OAAO;AACL,iBAAa;AAAA,EACf;AAEA,MAAI,CAAC,OAAO,QAAQ;AAClB,WAAO,SAAS,CAAC;AACjB,iBAAa;AAAA,EACf;AACA,QAAM,SAAS,OAAO;AACtB,MAAI,CAAC,OAAO,WAAW;AACrB,WAAO,YAAY,CAAC;AACpB,iBAAa;AAAA,EACf;AAEA,QAAM,YAAY,aAAa;AAC/B,QAAM,kBAAkB,oBAAoB,SAAS;AACrD,QAAM,YAAY,OAAO;AAEzB,MAAI,CAAC,UAAU,UAAU,GAAG;AAC1B,cAAU,UAAU,IAAI;AAAA,MACtB,SAAS;AAAA,MACT,KAAK;AAAA,MACL,QAAQ;AAAA,MACR,QAAQ;AAAA,IACV;AACA,iBAAa;AAAA,EACf,OAAO;AACL,UAAM,OAAO,UAAU,UAAU;AACjC,QAAI,QAAQ;AACZ,QAAI,CAAC,KAAK,WAAW,KAAK,YAAY,iBAAiB;AACrD,WAAK,UAAU;AACf,cAAQ;AAAA,IACV;AACA,QAAI,CAAC,KAAK,KAAK;AACb,WAAK,MAAM;AACX,cAAQ;AAAA,IACV;AACA,QAAI,CAAC,KAAK,QAAQ;AAChB,WAAK,SAAS;AACd,cAAQ;AAAA,IACV;AACA,UAAM,gBAAgB,KAAK;AAC3B,QAAI,CAAC,iBAAiB,CAAC,MAAM,QAAQ,aAAa,KAAK,cAAc,WAAW,gBAAgB,QAAQ;AACtG,WAAK,SAAS;AACd,cAAQ;AAAA,IACV;AACA,QAAI,MAAO,cAAa;AAAA,EAC1B;AAGA,MAAI,CAAC,OAAO,QAAQ;AAClB,WAAO,SAAS,CAAC;AACjB,iBAAa;AAAA,EACf;AACA,QAAM,SAAS,OAAO;AACtB,MAAI,CAAC,OAAO,UAAU;AACpB,WAAO,WAAW,CAAC;AACnB,iBAAa;AAAA,EACf;AACA,QAAM,WAAW,OAAO;AACxB,MAAI,CAAC,SAAS,OAAO;AACnB,aAAS,QAAQ,CAAC;AAClB,iBAAa;AAAA,EACf;AACA,QAAM,QAAQ,SAAS;AACvB,MAAI,CAAC,MAAM,SAAS;AAClB,UAAM,UAAU;AAChB,iBAAa;AAAA,EACf;AAGA,QAAM,cAAc;AAAA,IAClB,EAAE,IAAI,QAAQ,OAAO,OAAO;AAAA,IAC5B,EAAE,IAAI,UAAU,OAAO,SAAS;AAAA,IAChC,EAAE,IAAI,QAAQ,OAAO,OAAO;AAAA,IAC5B,EAAE,IAAI,SAAS,OAAO,QAAQ;AAAA,IAC9B,EAAE,IAAI,OAAO,OAAO,MAAM;AAAA,IAC1B,EAAE,IAAI,SAAS,OAAO,QAAQ;AAAA,IAC9B,EAAE,IAAI,YAAY,OAAO,WAAW;AAAA,IACpC,EAAE,IAAI,QAAQ,OAAO,OAAO;AAAA,IAC5B,EAAE,IAAI,QAAQ,OAAO,OAAO;AAAA,IAC5B,EAAE,IAAI,YAAY,OAAO,WAAW;AAAA,EACtC;AAEA,MAAI,CAAC,SAAS,QAAQ;AACpB,aAAS,SAAS,CAAC;AACnB,iBAAa;AAAA,EACf;AACA,QAAM,YAAY,SAAS;AAC3B,aAAW,KAAK,aAAa;AAC3B,UAAM,SAAS,YAAY,EAAE,EAAE;AAC/B,QAAI,CAAC,UAAU,MAAM,GAAG;AACtB,gBAAU,MAAM,IAAI,EAAE,OAAO,EAAE,MAAM;AACrC,mBAAa;AAAA,IACf;AAAA,EACF;AAEA,MAAI,YAAY;AACd,QAAI;AACF,oBAAc,YAAY,KAAK,UAAU,QAAQ,MAAM,CAAC,CAAC;AACzD,aAAO,KAAK,6CAA6C;AAAA,IAC3D,QAAQ;AAAA,IAER;AAAA,EACF;AACF;AAKA,eAAe,qBAA+D;AAC5E,SAAO;AAAA,IACL,MAAM;AAAA,IACN,aAAa;AAAA,IACb,aAAa;AAAA,IACb,aAAa;AAAA,IACb,SAAS,OAAO,QAA8B;AAC5C,YAAM,OAAO,SAAS,IAAI,MAAM,KAAK,KAAK,KAAK,EAAE,KAAK;AACtD,UAAI;AACF,cAAM,QAAQ,MAAM,SAAS,KAAK,IAAI,MAAM,EAAE,CAAC;AAC/C,eAAO,EAAE,MAAM,CAAC,OAAO,iBAAiB,KAAK,GAAG,KAAK,EAAE,KAAK,IAAI,EAAE;AAAA,MACpE,SAAS,KAAK;AACZ,eAAO;AAAA,UACL,MAAM,yBAAyB,eAAe,QAAQ,IAAI,UAAU,OAAO,GAAG,CAAC;AAAA,UAC/E,SAAS;AAAA,QACX;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;AAKA,eAAe,uBAAuB,KAAkE;AACtG,SAAO;AAAA,IACL,MAAM;AAAA,IACN,aAAa;AAAA,IACb,aAAa;AAAA,IACb,aAAa;AAAA,IACb,SAAS,YAAY;AACnB,YAAM,UAAU,mBAAmB;AACnC,YAAM,YAAY,OAAO,QAAQ,OAAO,EACrC,OAAO,CAAC,CAAC,GAAG,GAAG,MAAM,OAAO,OAAO,QAAQ,YAAY,YAAY,OAAO,IAAI,MAAM,EACpF,IAAI,CAAC,CAAC,MAAM,GAAG,MAAM;AACpB,cAAM,SAAS;AACf,cAAM,MAAM,OAAO,UAAU;AAC7B,cAAM,SAAS,IAAI,SAAS,IAAI,GAAG,IAAI,MAAM,GAAG,CAAC,CAAC,MAAM,IAAI,MAAM,EAAE,CAAC,KAAK;AAC1E,eAAO,YAAO,IAAI,SAAS,MAAM;AAAA,MACnC,CAAC;AAEH,UAAI,UAAU,WAAW,GAAG;AAC1B,eAAO;AAAA,UACL,MAAM;AAAA,YACJ;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,UACF,EAAE,KAAK,IAAI;AAAA,QACb;AAAA,MACF;AAEA,aAAO;AAAA,QACL,MAAM;AAAA,UACJ;AAAA,UACA;AAAA,UACA,GAAG;AAAA,UACH;AAAA,UACA,KAAK,UAAU,MAAM;AAAA,QACvB,EAAE,KAAK,IAAI;AAAA,MACb;AAAA,IACF;AAAA,EACF;AACF;AAGA,IAAM,SAAmC;AAAA,EACvC,IAAI;AAAA,EACJ,MAAM;AAAA,EACN,aAAa;AAAA,EACb,SAAS;AAAA,EAET,SAAS,KAAwB;AAC/B,UAAM,aAAa,QAAQ,IAAI,sBAAsB,UAAU,QAAQ,IAAI,sBAAsB;AACjG,QAAI,YAAY;AACd,UAAI,OAAO,KAAK,2CAA2C;AAC3D;AAAA,IACF;AAGA,QAAI,iBAAiB,GAAG;AACtB,UAAI,iBAAiB,eAAe;AACpC;AAAA,IACF;AAGA,UAAM,EAAE,UAAU,IAAI,mBAAmB,IAAI,gBAAgB,CAAC,CAAC;AAC/D,UAAM,UAAU,mBAAmB;AAEnC,UAAM,mBAAmB,OAAO,QAAQ,SAAS,EAC9C,OAAO,CAAC,CAAC,GAAG,GAAG,MAAM,IAAI,OAAO,EAChC,IAAI,CAAC,CAAC,QAAQ,MAAM,QAAQ;AAE/B,QAAI,iBAAiB,WAAW,GAAG;AACjC,UAAI,OAAO,KAAK,iEAAiE;AACjF;AAAA,IACF;AAEA,QAAI,OAAO,KAAK,kCAAkC,iBAAiB,KAAK,IAAI,CAAC,EAAE;AAG/E,QAAI,iBAAiB,eAAe;AAGpC,uBAAmB,IAAI,MAAM;AAG7B,UAAM,cAAc,aAAa;AACjC,QAAI,CAAC,IAAI,OAAO,OAAQ,KAAI,OAAO,SAAS,EAAE,WAAW,CAAC,EAAE;AAC5D,QAAI,CAAC,IAAI,OAAO,OAAO,UAAW,KAAI,OAAO,OAAO,YAAY,CAAC;AACjE,QAAI,OAAO,OAAO,UAAU,UAAU,IAAI;AAAA,MACxC,SAAS,oBAAoB,WAAW;AAAA,MACxC,KAAK;AAAA,MACL,QAAQ;AAAA,MACR,QAAQ;AAAA,IACV;AAGA,QAAI,CAAC,IAAI,OAAO,OAAQ,KAAI,OAAO,SAAS,CAAC;AAC7C,UAAM,SAAS,IAAI,OAAO;AAC1B,QAAI,CAAC,OAAO,SAAU,QAAO,WAAW,CAAC;AACzC,UAAM,WAAW,OAAO;AACxB,QAAI,CAAC,SAAS,MAAO,UAAS,QAAQ,CAAC;AACvC,UAAM,QAAQ,SAAS;AACvB,QAAI,CAAC,MAAM,QAAS,OAAM,UAAU;AAEpC,QAAI,OAAO,KAAK,uBAAuB,iBAAiB,MAAM,aAAa;AAG3E,uBAAmB,EAChB,KAAK,CAAC,QAAQ,IAAI,gBAAgB,GAAG,CAAC,EACtC,MAAM,MAAM;AAAA,IAAC,CAAC;AACjB,2BAAuB,GAAG,EACvB,KAAK,CAAC,QAAQ,IAAI,gBAAgB,GAAG,CAAC,EACtC,MAAM,MAAM;AAAA,IAAC,CAAC;AAGjB,QAAI,gBAAgB;AAAA,MAClB,IAAI;AAAA,MACJ,OAAO,YAAY;AACjB,cAAM,iBAAiB,GAAG;AAAA,MAC5B;AAAA,MACA,MAAM,YAAY;AAChB,YAAI,mBAAmB;AACrB,cAAI;AACF,kBAAM,kBAAkB,MAAM;AAAA,UAChC,QAAQ;AAAA,UAER;AACA,8BAAoB;AAAA,QACtB;AAAA,MACF;AAAA,IACF,CAAC;AAGD,qBAAiB,GAAG,EAAE,MAAM,CAAC,QAAQ;AACnC,UAAI,OAAO,MAAM,0BAA0B,eAAe,QAAQ,IAAI,UAAU,OAAO,GAAG,CAAC,EAAE;AAAA,IAC/F,CAAC;AAAA,EACH;AACF;AAEA,IAAO,gBAAQ;","names":["DEFAULT_PORT","getProxyPort","provider","modelId","apiKey","getProxyPort"]}